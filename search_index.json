[["index.html", "Analyse von Verhaltensdaten mit R Teil 1 Übersicht 1.1 Wozu dieses Dokument dient 1.2 Ergänzende Quellen 1.3 Hinweis zu diesem Dokument 1.4 Interaktive Verwendungsweise 1.5 Rückmeldung zu der Lernerfahrung 1.6 Die nächsten Schritte", " Analyse von Verhaltensdaten mit R Xaver Fuchs, Christian Seegelke, Tobias Heed Letzte Aktualisierung: 2021-12-10 Teil 1 Übersicht 1.1 Wozu dieses Dokument dient Dieses Dokument dient als Schablone für den Ablauf und die Auswertung von BSc/MSc-Arbeiten sowie in Auswertungen im Rahmen eines Empirischen Seminars (ES) in dem Reach &amp; Touch Lab / Kognitive Psychologie. Es enthält Hinweise zur Logik der Auswertung und Experimentaldesign, Wahl der statistischen Verfahren, zeitlichen Ablauf der Asrbeit, und konkrete Code-Beispiele. Viele Arbeiten werden an einzelnen Stellen von der Schablone abweichen müssen - sie dient nur als Leitfaden. Bitte in jedem Fall mit der Betreuer*in die einzelnen Punkte durchsprechen (ggf. auch Abweichungen vom hier dargestellten Vorgehen). 1.2 Ergänzende Quellen Es ist ausgesprochen nicht der Anspruch dieses Dokuments, eine umfassende Einführung zu geben. Es soll eher eine Art “Starthilfe” sein. Die hier vorkommenden Themen sind komplex und jedes Kapitel kann für sich dedizierte Bücher füllen–und tut das auch. Es gibt einige hervorragende Quellen, zum Vertiefen, von denen hier einige genannt sein sollten. Das Thema Datenvisualisierung ist hervorragend in dem Buch von Hadley Wickham (Wickham 2009) beschrieben. Datenanalyse und Visualisierung mit dem sogenannten Tidyverse ist tiefgehend in Hadley Wickhams Buch “R for data science” beschrieben, das auch online aufrufbar ist über die Website von Tidyverse: https://www.tidyverse.org/learn/ Zum Thema Datentransformation (“data wrangling”) gibt es auch eine sehr anschauliche Serie von Blog-Einträgen auf dieser Website: https://suzan.rbind.io/2018/01/dplyr-tutorial-1/ Insgesamt findet man über die üblichen Suchmaschinen im Internet sehr viel gutes Material. 1.3 Hinweis zu diesem Dokument Dieses Dokument basiert auf RMarkdown. Markdown ist eine einfache Syntax zur Erstellung von HTML, PDF, and MS Word Dokumenten. RMarkown ist eine spezielle Variante von Markdown, bei der Markdown mit R Code kombiniert wird, um reproduzierbare Reports zu erzeugen. Sie können hier mehr über RMarkdown lesen: http://rmarkdown.rstudio.com. Die wohl ausführlichste Dokumentation zu RMarkdown ist in dem Buch von Yihui Xie (Xie 2015) zu finden. Der gleiche Autor hat ebenfalls eine umfassende Bechreibung, die online verfügbar ist: https://bookdown.org/yihui/rmarkdown/ 1.4 Interaktive Verwendungsweise Das was dabei für Sie nützlich und wichtig ist, ist dass Sie das Dokument vollständig herunterladen können und auch die dazugehörigen Daten. Das ist sogar exakt was wir Ihnen nahe legen, denn man lernt am besten mit einem “hands-on”-Ansatz. Das heißt, wir wollen Sie ermuntern, das Dokument nicht nur zu lesen, sondern es selbst in RStudio zu öffnen und den Code auf Ihrem eigenen Computer laufen zu lassen. Dabei können Sie den Code nämlich variieren und damit “rumspielen” und sehen, was passiert. So so macht es am meisten Spaß und so lernt man am besten R. 1.5 Rückmeldung zu der Lernerfahrung Dieses Dokument ist dynamisch und wächst immer weiter. Sie können uns gerne rückmelden, wenn etwas nicht korrekt oder unklar ist, nicht funktioniert oder in einer anderen Weise verbessert werden könnte. Für solches Feedback sind wir immer dankbar. Senden Sie dieses Feedback gerne an Xaver Fuchs: xaver.fuchs@plus.ac.at 1.6 Die nächsten Schritte Zunächst können Sie hier weiterlesen, wie Sie Ihren Computer überhaupt in die Lage versetzen, dass Sie damit mit R und RStudio arbeiten können. Danach werden wir Schritt für Schritt verschiedenen Datenverarbeitungsschritte, sowie das Erstellen von Abbildungen und das Durchführen von statistischen Analysen bearbeiten. References "],["get-started.html", "Teil 2 Einstieg 2.1 R und RStudio 2.2 Dokument mit ersten Schritten 2.3 Code und Daten herunterladen 2.4 Projektordner anlegen", " Teil 2 Einstieg 2.1 R und RStudio Zunächst brauchen Sie die Programme R und RStudio. Die Programme sind frei verfügbar und für die Plattformen Windows, Mac und Linux verfügbar. 2.2 Dokument mit ersten Schritten Wir haben ein pdf-Dokument kompiliert, das Sie durch die Installation und die ersten Schritte mit R führt. Wir empfehlen, dass zunächst dieses Dokument bearbeiten und danan zu diesem Dokument zurückkehren. Das Einführungsdokument finden Sie hier. 2.3 Code und Daten herunterladen Für die interaktive Arbeit mit diesem Dokument benötigen Sie das RMarkdown-Dokument, sowie vier Datensätze. Die Datensätze sind eigentlich identisch, aber in unterschiedlichen Formaten gespeichert. Wenn Sie die Browser-Version dieses Buches betrachten, gelangen Sie zu allen Resource (Daten und Code), wenn Sie ganz links oben in der Ecke auf “Auswerteschablone für R” klicken. 2.3.1 RMarkdown code Den Code für das gesamte Dokument heißt “Reach &amp; Touch Lab Auswerteschablone.Rmd.” Sie finde ihn wie oben beschrieben oder Sie können ihn auch hier herunterladen. In dieser Version sind alle Kapitel des Dokuments in einem Dokument. Um den Überblick zu behalten, benutzen Sie am besten die Outline-Funktion in RStudio. Diese finden Sie rechts oben im Skript-Fenster, wo steht “Show document outline” 2.3.2 Daten Hier können Sie die Daten herunterladen. Die Daten öffnen sich in einem neuen Tab, aber um Sie herunterzuladen müssten Sie im Kontextmenü (z.B. rechte Maustaste) auf “Ziel speichern unter…” o.Ä. gehen. Textfile: RTL_beispieldaten.txt als Excelfile: RTL_beispieldaten.xlsx als SPSS-File: RTL_beispieldaten_RTs.sav als RData-File: RTL_beispieldaten.RData Bitte laden Sie alle vier Datensätze herunter da sonst Teile des Codes nicht funktionieren werden. 2.4 Projektordner anlegen Legen Sie nun auf Ihrem Rechner einen Projektordner an und kopieren Sie das RMarkdown-Dokument und die vier Datensätze dort hin. "],["read-data.html", "Teil 3 Pakete und Daten laden 3.1 Verwendete Packages 3.2 Daten einlesen 3.3 Daten laden 3.4 Daten speichern", " Teil 3 Pakete und Daten laden 3.1 Verwendete Packages Man kann alle Packages zu Anfang laden, oder jeweils dann, wenn man sie im Skript braucht. 3.1.1 Pakete für Datenverarbeitung und Statistik Das vorliegende Dokument verwendet tidyverse, welches sehr viele sehr nützliche Funktionen für Datenauswertung bereithält. afex und emmeans benötigt man nur, wenn man Linear Mixed Models verwendet. Bei BSc/MSc Arbeiten ist dies der Fall, wenn Prozentwert-Daten ausgewertet werden müssen. rio ist ein nützliches Paket, das man braucht, wenn man Datensätze aus anderen Formaten (z.B. Excel oder SPSS) einlesen oder in solchen Formaten speichern will. Mit cowplot lassen sich leicht mehrere Abbildungen kombinieren. library( tidyverse ) library( afex ) library( emmeans ) library( rio ) library( cowplot ) 3.1.2 Pakete für RMarkdown Zusätzlich verwenden wir noch zwei Pakete, die dazu da sind, Tabellen schöner aussehen zu lassen. library( knitr ) library( kableExtra ) 3.2 Daten einlesen Als nächstes wollen wir die Daten in R einlesen, so dass sie im Workspace sind und man damit arbeiten kann. 3.2.1 Verzeichnisse und Pfade R hat immer einen “Ort,” an dem es auf Ihrem Computer ist. Das ist der Pfad (oder das Verzeichnis) von dem aus R “denkt.” Man kann unterscheiden zwischen absoluten Pfaden und relativen Pfaden. Ein Absoluter Pfad fängt mit einer Laufwerksbezeichnung an (auf Windows), zum Beispiel wäre “C:/Programming &amp; Stats stuff/Statistics Tutorial RTL/Daten/Datensatz1.csv” ein absoluter Pfad. Absolute Pfade haben den Nachteil, dass sie dann nicht mehr stimmen, wenn Sie mal etwas in dem Namen (z.B. den Namen eines Ordners) verändern. Ein relativer Pfad ist ein Pfad, der relativ formuliert ist zu dem Ort, an dem R gerade “ist” bzw. von wo aus R gerade “denkt.” Nehmen wir mal an, der Arbeitspfad (working direectory) von R wäre bei Ihnen gerade auf das Verzeichnis “C:/Programming &amp; Stats stuff/Statistics Tutorial RTL/” gesetzt, dann wäre der Pfad zu dem Datensatz von oben relativ dazu ausgedrückt “Daten/Datensatz1.csv.” Der Vorteil von relativen Pfaden ist, dass sie weiterhin stimmen, selbst wenn Sie den Ordner verschieben oder umbenennen, so lange der relative Ort bestehen bleibt (also der Ordern “Daten” mit Inhalt) auch mitverschoben wird. Ein Nachteil ist, dass man den Pfad dann am Anfang der Session mit R auf das gewünschte Verzeichnis setzen muss. Ein weiterer großer Vorteil von relativen Pfaden ist, dass Plots, Daten dann auch dort gespeichert (es sei denn man spezifiziert es beim Speichern anders). In der Summe ist das Arbeiten mit relativen Pfaden sinnvoller und wir werden daher in diesem Dokument immer mit relativen Pfaden arbeiten. 3.2.2 Verzeichnisse ermitteln und setzen 3.2.2.1 Verzeichnis ermitteln Sie können einfach herausfinden, wo R gerade “ist,” indem Sie folgenden Befehl verwenden: getwd() ## [1] &quot;/Users/b1082752/Nextcloud/RTL_GitHub/Auswerteschablone Rtut/Rtut_Bookdown&quot; RStudio hat eine sehr praktische Methode eingebaut, um herauszufinden, an welchem Ort sich das Skript oder Rmarkdown-Dokument befindet, in dem Sie aktuell arbeiten. Das kann sehr praktisch sein, da man oft das Arbeitsverzeichnis da hin setzen will, wo das Skript auch ist, um dann zum Beispiel von dort mit relativen Pfaden zu arbeiten. Dieser Befehl liest den Pfad des aktuellen Dokuments und schreibt ihn in die Variable “dataPath”: dataPath &lt;- dirname(rstudioapi::getActiveDocumentContext()$path) print(dataPath) ## [1] &quot;&quot; 3.2.2.2 Verzeichnis setzen Das Pendant zu “getwd()” heißt “setwd()” und ist dazu da, um ein Verzeichnis zu ändern. Sie können ganz einfach den Pfad “dataPath,” die oben definierte Variable erstellen. setwd(dataPath) Sie können natürlich den Befehl auch auf einen xbeliebigen anderen (absoluten) Pfad setzen. Zum Beispiel könnten Sie zunächst definieren: dataPath &lt;- &quot;C:/Programming &amp; Stats stuff/Statistics Tutorial RTL/&quot; setwd(dataPath) Oder auch gleich: setwd(&quot;C:/Programming &amp; Stats stuff/Statistics Tutorial RTL/&quot;) Ersetzen Sie dabei einfach den Pfad auf was auch immer in Ihrer Situation zutrifft. Hinweis: Bei Pfaden gehen die Striche je nach Betriebssystem in die andere Richtung: Mac und Linux verwenden ein /, Windows aber einen  als Verzeichnistrenner. 3.3 Daten laden Zeitplanung: nach den ersten 1-2 Erhebungen Die häufigsten Datenformate sind vermutlich: Textdateien, in denen jede Spalte eine Variable, und jede Zeile einen Trial enthält Exceldateien mit demselben Format bereits in R erstellte oder bearbeitete Daten, die im R-Format gespeichert wurden SPSS-Dateien, in denen repeated measures nicht über Codiervariablen, sondern über 1 Spalte je Bedingung gespeichert sind Für alle 4 Fälle wird hier Code zur Verfügung gestellt. 3.3.1 Daten, die als Textdatei vorliegen Beim Einlesen von sogenannten comma separated values (csv) Dateien, worunter auch übliche Text-Dateien fallen, ist zu beachten, dass diese Dateien bestimmte Trennzeichen haben können. Üblich sind durch einen Tabulator (Tab) getrennte Dateien, durch Komma getrennte Dateien oder durch Punkte getrennte Dateien. Dementsprechend muss man in der Funktion die Flag “sep” anpassen. Im Beispiel ist die Textdatei durch Tabs getrennt, deshalb funktioniert sep = “ #daten laden bsp &lt;- read.csv(&quot;RTL_beispieldaten.txt&quot;, sep = &quot;\\t&quot;) Die Daten sehen in etwa so aus (Ausschnitt) bsp %&gt;% select(Participant, Stimuli, Condition, Repetition, SOA, Response.correct, Real_Release_RT) %&gt;% slice(1:10) %&gt;% kable(caption = &#39;Ausschnitt der Beispieldaten&#39;, booktabs = TRUE) Table 3.1: Ausschnitt der Beispieldaten Participant Stimuli Condition Repetition SOA Response.correct Real_Release_RT B006b-08 6_1 uncrossed 1 800 TRUE 618 B006b-08 5_2 uncrossed 1 100 TRUE 824 B006b-08 6_2 uncrossed 1 50 TRUE 752 B006b-08 6_1 uncrossed 1 300 TRUE 646 B006b-08 2_5 crossed 1 800 TRUE 869 B006b-08 1_6 crossed 1 50 TRUE 936 B006b-08 1_3 crossed 1 50 TRUE 544 B006b-08 1_5 uncrossed 1 300 TRUE 1029 B006b-08 4_1 uncrossed 1 100 TRUE 779 B006b-08 2_5 uncrossed 1 800 TRUE 504 3.3.2 Daten, die in Excel vorliegen bsp &lt;- import(&quot;RTL_beispieldaten.xlsx&quot;, which = 1) Dieser Befehl kommt aus dem Paket riio und kann für Exceltabellen auch noch andere Parameter annehmen, z.B. für das Excelblatt (bei Daten mit mehreren Blättern) kann man das zweite Blatt mit “which = 2” einlesen. 3.3.3 Daten, die in SPSS vorliegen Das Paket rio kann auch verwendet werden, um SPSS-Daten einzulesen. bsp_RTs&lt;- import(&quot;RTL_beispieldaten_RTs.sav&quot;) 3.3.4 Daten, die schon in R gespeichert waren Hier kann es sein, dass mehr drin ist als man braucht. Also erstmal anschauen. Dazu den Workspace leeren (in RStudio oben rechts bei Reiter “Environment” den kleinen Besen klicken - das löscht alle vorhandenen Daten). Anschließend den load-Befehl ausführen. Das, was dann im “Global Environment” gezeigt wird, sind die Daten, die man geladen hat. load( &quot;RTL_beispieldaten.RData&quot; ) 3.4 Daten speichern Sobald Daten in R importiert wurden, sollten sie im R-Format abgespeichert werden, da dann das Laden am einfachsten ist und alle bearbeiteten Variablen ihre Eigenschaften (z.B. Variablentyp) behalten. Dies gilt insbesondere dann, wenn man die Daten aus einem anderen Format (txt, SPSS) nach R holt. Wenn Sie an den Daten etwas verändert haben, passen Sie auf, dass Sie nicht ihre ursprünglichen Daten aus Versehen überschreiben. Das können Sie verhindern, indem Sie dem Datensatz einen neuen Namen geben. In unserem Beispiel hängen wir “_export” hinten an den Dateinamen dran. 3.4.1 Speichern im R-typischen Format (als RData) # Daten für weitere Verwendung in R speichern, zB um Sie seine*r Betreuer*in zu schicken save( bsp, file = &quot;RTL_beispieldaten_export.RData&quot;) 3.4.2 Speichern als Textdatei write.table(bsp, file = &quot;RTL_beispieldaten_export.txt&quot;, sep = &quot;\\t&quot;, row.names = FALSE, col.names = TRUE) “sep” ist das Trennzeichen zwischen Spalten, t ist ein Tab-zeichen, das ist Standard. “col.names” exportiert auch die Variablennamen als erste Spalte 3.4.3 Speichern als als Excel-Datei Hierzu kann der “export”-Befehl aus dem Paket rio verwendet werden. export(bsp, file = &quot;RTL_beispieldaten_export.xlsx&quot;) 3.4.4 Speichern als SPSS-Datei Auch das SPSS-Format kann mit “export” wie bei den Excel-Dateien gespecihert werden. Bei SPSS ist das weite Format üblich. Da es aber schnell unübersichtlich wird, wird hier beispielhaft nur ein Ausschnitt verwendet, der nur die Reaktionszeiten (RTs) enthält. Die verwendeten Befehle zum Erstellen des Ausschnitts werden unten noch genauer erklärt (und müssen hier nicht im Detail nachvollzogen werden) # Reaktionszeiten über die VP und Reize aggregieren (mitteln) bsp_RTs &lt;- bsp %&gt;% filter(Repetition==1) %&gt;% dplyr::select(Participant, Stimuli, SOA, Real_Release_RT) %&gt;% group_by(Participant, Stimuli, SOA) %&gt;% summarise(RT=round(mean(as.numeric(Real_Release_RT), na.rm = T), digits = 2)) %&gt;% ungroup() ## `summarise()` has grouped output by &#39;Participant&#39;, &#39;Stimuli&#39;. You can override using the `.groups` argument. # den Auschnitt in eine &quot;weites&quot; Format (SPSS-typisch) umwandeln... bsp_RTs_wide &lt;- bsp_RTs %&gt;% unite(Stim_SOA, c(Stimuli, SOA)) %&gt;% pivot_wider(names_from = Stim_SOA, values_from = RT) Die Daten im weiten Format sehen dann so aus (Ausschnitt) #Datensatz betrachten: Jede VP ist jetzt in einer Zeile (=weites Format, s.u.) bsp_RTs_wide %&gt;% select(1:10) %&gt;% slice(1:10) %&gt;% kable(caption = &#39;Ausschnitt der Beispieldaten im weiten Format&#39;) %&gt;% scroll_box(width = &quot;100%&quot;) Table 3.2: Ausschnitt der Beispieldaten im weiten Format Participant 1_3_50 1_3_100 1_3_300 1_3_800 1_4_50 1_4_100 1_4_300 1_4_800 1_5_50 B006b-08 743.00 747.92 733.92 632.71 924.33 716.83 647.58 578.71 872.04 B006b-09 802.79 720.42 567.62 572.75 686.54 668.83 613.62 567.92 797.50 B006b-10 1117.22 976.00 887.75 928.29 1122.54 1087.75 923.58 864.96 1107.52 B006b-11 1144.91 1020.50 1101.33 892.58 1096.71 960.54 965.83 936.04 1112.96 B006b-12 934.12 786.08 617.50 693.38 836.12 803.46 779.08 725.23 925.09 B006b-13 672.25 683.50 599.79 668.17 950.77 813.79 719.17 601.46 617.67 B006b-14 1318.33 1354.79 1271.75 1063.04 1112.21 1114.29 915.42 877.22 1571.54 B006b-15 676.41 690.81 858.58 866.30 835.46 763.48 807.00 840.59 663.43 B006b-16 1011.29 922.17 935.17 812.08 1099.46 1065.29 985.88 933.71 1023.21 B006b-17 852.88 701.50 741.35 601.09 870.79 805.88 721.65 692.96 687.25 Bevor wir als SPSS-Datei speichern, müssen wir die Variablen auch noch ein bisschen umbenenen, weil SPSS es nicht mag, wenn Variablennamen mit einer Zahl beginnen. Da es alles RTs sind, hängen wir einfach mal “RT” vorne an die Namen dran VarsOldName &lt;- grep(&quot;[[:digit:]]&quot;, names(bsp_RTs_wide), value = T) VarsNewName &lt;- paste(&quot;RT&quot;, VarsOldName, sep=&quot;s_&quot;) bsp_RTs_wide &lt;- bsp_RTs_wide %&gt;% rename_at(vars(VarsOldName), ~ VarsNewName) export(bsp_RTs_wide, &quot;RTL_beispieldaten_export_RTs.sav&quot;) #exportiert als SPSS-Datei "],["plan-analysis.html", "Teil 4 Analysen planen und Variablentypen anpassen 4.1 Die richtige Analyse für das vorliegende Design wählen 4.2 Daten ins richtige Format bringen 4.3 Variablen von Interesse auswählen 4.4 Ergebnis überprüfen", " Teil 4 Analysen planen und Variablentypen anpassen 4.1 Die richtige Analyse für das vorliegende Design wählen Bevor man mit den Daten losgegt, sollte man sich gut überlegen, wie das Experimentaldesign aussieht und in die Analyse umgesetzt werden muss. Dazu gehört u.a.: Was ist/ sind meine abhängige Variable/n (AV)? Welches Skalenniveau (nominal, ordinal, kardinal) hat/ haben meine AV? Beachte Abschnitt “Umgang mit %-Daten” weiter unten Was ist sind meine unabhängige Variable/n (UV)? Dies sind i.d.R. unsere experimentellen Faktoren. Welches Skalenniveau hat/ haben meine UV? Welche Ausprägungen/ wieviel Stufen haben meine UV? Handelt es sich um Innersubjektfaktoren (within-subject, abhängige Messungen) und/ oder Zwischensubjektfakoren (between-subject, unabhängige Messungen)? Sind die entsprechenden Variablen in R im korrekten Format (diskrete UV sollten als “Factor” vorliegen, kontinuierliche als “int” (=integer)) oder “numeric.” Um die Variablen ins richtige Format zu bringen, s. Abschnitt “Daten ins richtige Format bringen” Sind alle Variablen, die ich benötige schon vorhanden, oder müssen ggf. neue Variablen berechnet werden? 4.2 Daten ins richtige Format bringen Zeitplanung: nach den ersten 1-2 Erhebungen Nachdem die Daten geladen sind, muss man sie so bearbeiten, dass man sie richtig nutzen kann. Gründe dafür sind, z.B.: manche Variablen haben möglicherweise ein ungünstiges Format (z.B. s statt ms) R weiß noch nicht, welche Variablen welchen Typ haben; insbesondere muss man R sagen, welche Variablen Faktoren sind, da sonst viele statistische Auswertungen nicht funktionieren. Was muss man dabei typischerweise beachten? Alles, was man aus einer Textdatei an Texten geladen hat, sind in R zunächst “chr” (steht für character). Dies sind normalerweise in unserer Experimentallogik Faktoren: Vp-Bezeichnungen =&gt; eine Vp ist ein Faktorlevel auf dem Faktor “subject” (bei uns oft subjNr), denn eine Vp wird vielleicht nicht numerisch als 1, 2, … bezeichnet, sondern komplexer bspw. als B006_s01. Daten wie “gekreuzt, ungekreuzt” - es ist sinnvoll, wenn man die im Datensatz als Wort belässt; nur muss R wissen, dass man damit Bedingungen meint (und nicht zB einzelne Wörter) Sonderfall, wenn man unterschiedliche SOA verwendet und diese in die Analyse aufnehmen will: dies sind ja Zahlen (zB 100, 200, 300 ms zwischen einem Prime und einem Stimulus); aber in einer Anova sind es Faktorstufen. Darum muss man sie ebenfalls als Faktor codieren. Häufig enthalten Datenfiles allerlei Spalten, die man gar nicht braucht. Darum erstellt man für Übersichtlichkeit am Schluss ein data.frame, das nur die Sachen enthält, die man wirklich braucht. Im Folgenden, werden wir step-by-step durch diese Schritte durchgehen. 4.2.1 Überblick über Datensatz verschaffen Zunächst muss man sich orientieren: was hat man eigentlich geladen?? Im Folgenden wird angenommen, dass die .RData Daten geladen wurden. Im “Environment” bei RStudio sieht man, dass es ein data.frame “bsp” gibt. Mit dem Befehl “head” stellt man die ersten Zeilen dar. head( bsp ) ## Participant Tactor1 Tactor2 Stimuli Time Condition Repetition Trialnumber Trialtype SOA Stim1Intens Stim2Intens ITI TimeAfterBeep Response Release_RT ## 1 B006b-08 6 1 6_1 2018-11-27-15-56-08 uncrossed 1 163 360 800 90 75 1000 996 3_4 1433 ## 2 B006b-08 5 2 5_2 2018-11-27-14-30-16 uncrossed 1 170 378 100 90 80 1000 612 3_4 939 ## 3 B006b-08 6 2 6_2 2018-11-27-15-54-17 uncrossed 1 138 313 50 100 75 1000 969 1_2 817 ## 4 B006b-08 6 1 6_1 2018-11-27-14-24-41 uncrossed 1 91 343 300 100 70 1000 821 3_4 961 ## 5 B006b-08 2 5 2_5 2018-11-27-16-17-10 crossed 1 58 276 800 75 100 1000 721 3_4 1684 ## 6 B006b-08 1 6 1_6 2018-11-27-15-16-24 crossed 1 305 245 50 80 90 1000 656 3_4 1001 ## Real_Release_RT RT1_press_raw RT1_release_raw RT2_press_raw RT2_release_raw RT3_press_raw RT3_release_raw RT4_press_raw RT4_release_raw Trial_ok Trial_repeated Reason ## 1 618 [] [] [] [] 1746 1475 1739 1433 True False No reason ## 2 824 [] [] [] [] 1272 960 1299 939 True False No reason ## 3 752 1115 824 1124 817 [] [] [] [] True False No reason ## 4 646 [] [] [] [] 1308 963 1336 961 True False No reason ## 5 869 [] [] [] [] 1985 1702 2000 1684 True False No reason ## 6 936 [] [] [] [] 1318 1017 1326 1001 True False No reason ## LoadSucess Run Tactor1Side Tactor2Side Tactor1Segment Tactor2Segment StimSide StimSegment Order ResponseCoding Response.meaning Response.correct ## 1 success Run2 left right upper lower different different uncrossed first 1_2 (Zehen hoch) means same different TRUE ## 2 success Run1 right left upper lower different different uncrossed first 1_2 (Zehen hoch) means same different TRUE ## 3 success Run2 left left upper lower same different uncrossed first 1_2 (Zehen hoch) means same same TRUE ## 4 success Run1 left right upper lower different different uncrossed first 1_2 (Zehen hoch) means same different TRUE ## 5 success Run2 left right lower upper different different uncrossed first 1_2 (Zehen hoch) means same different TRUE ## 6 success Run1 right left lower upper different different uncrossed first 1_2 (Zehen hoch) means same different TRUE Man sieht, dass da sehr viel Zeug drin ist. Mit “str” kann man sich die Struktur des Datensatzes anschauen, d.h., herausfinden was für Variablen er enthält und was für Typen die Variablen haben. Vorne steht der Variablenname, danach der Typ; z.B. bei Participant “chr,” also Text und noch nicht Faktor! Selbiges gilt für viele andere Variablen auch. Nach der Typangabe sieht man ein par Beispiele, was für Daten genau in der Variable drin sind. str( bsp ) ## &#39;data.frame&#39;: 32455 obs. of 40 variables: ## $ Participant : chr &quot;B006b-08&quot; &quot;B006b-08&quot; &quot;B006b-08&quot; &quot;B006b-08&quot; ... ## $ Tactor1 : int 6 5 6 6 2 1 1 1 4 2 ... ## $ Tactor2 : int 1 2 2 1 5 6 3 5 1 5 ... ## $ Stimuli : chr &quot;6_1&quot; &quot;5_2&quot; &quot;6_2&quot; &quot;6_1&quot; ... ## $ Time : chr &quot;2018-11-27-15-56-08&quot; &quot;2018-11-27-14-30-16&quot; &quot;2018-11-27-15-54-17&quot; &quot;2018-11-27-14-24-41&quot; ... ## $ Condition : chr &quot;uncrossed&quot; &quot;uncrossed&quot; &quot;uncrossed&quot; &quot;uncrossed&quot; ... ## $ Repetition : chr &quot;1&quot; &quot;1&quot; &quot;1&quot; &quot;1&quot; ... ## $ Trialnumber : int 163 170 138 91 58 305 224 203 117 123 ... ## $ Trialtype : int 360 378 313 343 276 245 21 203 162 272 ... ## $ SOA : int 800 100 50 300 800 50 50 300 100 800 ... ## $ Stim1Intens : int 90 90 100 100 75 80 70 75 80 80 ... ## $ Stim2Intens : int 75 80 75 70 100 90 85 100 80 90 ... ## $ ITI : int 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 ... ## $ TimeAfterBeep : int 996 612 969 821 721 656 592 786 684 504 ... ## $ Response : chr &quot;3_4&quot; &quot;3_4&quot; &quot;1_2&quot; &quot;3_4&quot; ... ## $ Release_RT : chr &quot;1433&quot; &quot;939&quot; &quot;817&quot; &quot;961&quot; ... ## $ Real_Release_RT : chr &quot;618&quot; &quot;824&quot; &quot;752&quot; &quot;646&quot; ... ## $ RT1_press_raw : chr &quot;[]&quot; &quot;[]&quot; &quot;1115&quot; &quot;[]&quot; ... ## $ RT1_release_raw : chr &quot;[]&quot; &quot;[]&quot; &quot;824&quot; &quot;[]&quot; ... ## $ RT2_press_raw : chr &quot;[]&quot; &quot;[]&quot; &quot;1124&quot; &quot;[]&quot; ... ## $ RT2_release_raw : chr &quot;[]&quot; &quot;[]&quot; &quot;817&quot; &quot;[]&quot; ... ## $ RT3_press_raw : chr &quot;1746&quot; &quot;1272&quot; &quot;[]&quot; &quot;1308&quot; ... ## $ RT3_release_raw : chr &quot;1475&quot; &quot;960&quot; &quot;[]&quot; &quot;963&quot; ... ## $ RT4_press_raw : chr &quot;1739&quot; &quot;1299&quot; &quot;[]&quot; &quot;1336&quot; ... ## $ RT4_release_raw : chr &quot;1433&quot; &quot;939&quot; &quot;[]&quot; &quot;961&quot; ... ## $ Trial_ok : chr &quot;True&quot; &quot;True&quot; &quot;True&quot; &quot;True&quot; ... ## $ Trial_repeated : chr &quot;False&quot; &quot;False&quot; &quot;False&quot; &quot;False&quot; ... ## $ Reason : chr &quot;No reason&quot; &quot;No reason&quot; &quot;No reason&quot; &quot;No reason&quot; ... ## $ LoadSucess : chr &quot;success&quot; &quot;success&quot; &quot;success&quot; &quot;success&quot; ... ## $ Run : chr &quot;Run2&quot; &quot;Run1&quot; &quot;Run2&quot; &quot;Run1&quot; ... ## $ Tactor1Side : Factor w/ 2 levels &quot;left&quot;,&quot;right&quot;: 1 2 1 1 1 2 2 2 1 1 ... ## $ Tactor2Side : Factor w/ 2 levels &quot;left&quot;,&quot;right&quot;: 2 1 1 2 2 1 2 2 2 2 ... ## $ Tactor1Segment : Factor w/ 2 levels &quot;lower&quot;,&quot;upper&quot;: 2 2 2 2 1 1 1 1 1 1 ... ## $ Tactor2Segment : Factor w/ 2 levels &quot;lower&quot;,&quot;upper&quot;: 1 1 1 1 2 2 1 2 1 2 ... ## $ StimSide : chr &quot;different&quot; &quot;different&quot; &quot;same&quot; &quot;different&quot; ... ## $ StimSegment : chr &quot;different&quot; &quot;different&quot; &quot;different&quot; &quot;different&quot; ... ## $ Order : chr &quot;uncrossed first&quot; &quot;uncrossed first&quot; &quot;uncrossed first&quot; &quot;uncrossed first&quot; ... ## $ ResponseCoding : chr &quot;1_2 (Zehen hoch) means same&quot; &quot;1_2 (Zehen hoch) means same&quot; &quot;1_2 (Zehen hoch) means same&quot; &quot;1_2 (Zehen hoch) means same&quot; ... ## $ Response.meaning: chr &quot;different&quot; &quot;different&quot; &quot;same&quot; &quot;different&quot; ... ## $ Response.correct: logi TRUE TRUE TRUE TRUE TRUE TRUE ... Wir sehen, dass bei Participant ziemlich lange Namen drin sind. Da sie informativ sind, wollen wir sie auch gern behalten. Dies ist kein Problem, weil Faktor-Level lange Namen haben dürfen. Hingegen ist es für Abbildungen oft praktischer, wenn Faktoren nicht so lange Namen haben, denn die sind dann schlecht lesbar in der Abbildung. Wir werden unten daher “uncrossed” in der Variable “Condition” umbenennen. 4.2.2 Kopie des originalen Datensatzes machen Weil man häufiger zu dem originalen Datensatz zurückkehren möchte (z.B., weil man später doch noch eine Variable braucht, die man vorher schon eliminiert hatte), empfehlt es sich, den originalen Datensatz im Workspace zu behalten und zum Bearbeiten lieber eine Kopie anzulegen. Dazu kopiert man ihn einfach in eine neue Variable. Weil man nicht so viel tippen will, kann man seinen Datensatz am besten sehr kurz benennen. Wir nennen ihn einfach mal “d.” Das hat den Nebeneffekt, dass man immer vergleichen kann, wie die Daten am Anfang aussahen (bsp) und wie sie aussehen, nachdem man etwas dran gemacht hat (d). d &lt;- bsp 4.2.3 Variablen und Faktorlevels umbenennen Zu 1a und 1b: Faktoren machen und richtig ordnen R sortiert von sich aus alphabetisch. Diese Reihenfolge wird dann in Grafiken auch verwendet. Um das so zu haben, wie man es sich wünscht, sortiert man selbst. Da für das Beispiel der Inhalt des Experiments egal ist, benennen wir die Faktorlevel in Level 1 und Level 2 um. Wir belassen nur uncrossed/crossed, weil das bei unseren Datensätzen oft vorkommt. d$Participant &lt;- as.factor( d$Participant ) # Faktor machen d &lt;- d %&gt;% mutate( Condition = recode_factor( as.factor( Condition ), &quot;uncrossed&quot; = &quot;uncr&quot;, &quot;crossed&quot; = &quot;crossed&quot; ) ) # vor dem = der alte Name, dahinter der neue d &lt;- d %&gt;% mutate( StimSide = recode_factor( as.factor( StimSide ), &quot;same&quot; = &quot;Level_1&quot;, &quot;different&quot; = &quot;Level_2&quot; ) ) d &lt;- d %&gt;% mutate( StimSegment = recode_factor( as.factor( StimSegment ), &quot;same&quot; = &quot;Level_1&quot;, &quot;different&quot; = &quot;Level_2&quot; ) ) zu 1c: Das Bsp-Experiment hatte auch verschiedene SOAs. Zunächst prüfen wir, welche SOAs vorliegen. str( d$SOA ) ## int [1:32455] 800 100 50 300 800 50 50 300 100 800 ... Wir schauen mal rein, was für Werte drin sind: Integer-Zahlen 50, 100, 300, 800 Wir erstellen eine zweite Variable, die dieselben Daten aufnimmt. d$soaNum &lt;- d$SOA Anschließend machen wir aus der alten Variable einen Faktor. d$SOA &lt;- as.factor(d$SOA) Warum macht man das? Wenn man SOA in einer ANOVA verwenden will, dann muss man es als Faktor eingeben. Manchmal möchte man Daten aber so plotten, dass die Abstände zwischen den Punkten den echten Wert widerspiegeln. Dazu muss man dann die Integer-Zahlen nutzen. Weil die Integer-Variable jetzt “num” heißt, weiß man, dass man die nur dann nimmt, wenn man keinen Faktor haben will. Wenn man diesen Schritt (Faktor machen) vergisst, erhält man seltsame Fehlermeldungen in ezAnova oder afex. Man muss sich auf diese Weise alle Daten ansehen und überlegen, welche Spalten man braucht, ob sie im richtigen Format sind etc. 4.2.4 Abhängige Variablen prüfen und in richtigers Format bringen Unsere abhängigen Variablen sind, (1) ob eine die korrekte Antwort gegeben wurde udn (2) die Reaktionszeit. 4.2.4.1 AV Antwortverhalten Die Kodierung für die Antworten liegt noch nicht so vor, wie wir sie gern hätten. str( d$Response.correct ) ## logi [1:32455] TRUE TRUE TRUE TRUE TRUE TRUE ... Da sind Wörter “TRUE, FALSE,” und Variablentyp “logi” = binär richtig/falsch. Wir erstellen daher eine neue Variable und füllen sie zunächst mit 1 für “richtig” oder “TRUE.” Überall da, wo gar nicht TRUE drinsteht, sondern FALSE, setzen wir dann eine 0 rein. d$resp &lt;- 1 #zunächst alles auf 1 d$resp[ d$Response.correct != &quot;TRUE&quot; ] &lt;- 0 #Fälle auf 0 4.2.4.2 AV Reaktionszeit Die RT ist auch nicht so, wie wir sie brauchen, denn der “str”-Befehl zeigt an, dass die Variable als “chr” formatiert ist. str(d$Real_Release_RT) ## chr [1:32455] &quot;618&quot; &quot;824&quot; &quot;752&quot; &quot;646&quot; &quot;869&quot; &quot;936&quot; &quot;544&quot; &quot;1029&quot; &quot;779&quot; &quot;504&quot; &quot;787&quot; &quot;553&quot; &quot;765&quot; &quot;507&quot; &quot;532&quot; &quot;798&quot; &quot;638&quot; &quot;812&quot; &quot;575&quot; &quot;510&quot; &quot;859&quot; &quot;695&quot; &quot;462&quot; &quot;776&quot; &quot;697&quot; ... d$rt &lt;-as.numeric( as.character( d$Real_Release_RT ) ) str(d$rt) ## num [1:32455] 618 824 752 646 869 ... Jetzt sind es Zahlen. 4.3 Variablen von Interesse auswählen Am Schluss erstellen wir einen neuen data.frame, der nur enthält, was wir wirklich brauchen. Damit wird im vorliegenden Fall aus einem data.frame mit 40 Spalten (sehr unübesichtlich) ein viel kürzerer. Dabei kann man auch gleich noch Namen umbenennen, die einem nicht gefallen. Z.B: ist die Bezeichnung “Condition” sehr ungünstig, weil sehr unspezifisch. Wir nennen sie hier “Posture.” Für das vorliegende Bsp. nennen wir die anderen Faktoren A und B, da für unser Bsp. egal ist, was da genau getestet wurde. Wieder machen wir (wie oben) eine Kopie des Datensatzes. Der neue Name “ds” steht für “data selected” und ist immer noch angenehm kurz. ds &lt;- d %&gt;% dplyr::select( participant = Participant, factor_A = StimSegment, factor_B = StimSide, posture = Condition, SOA, soaNum, resp, rt ) 4.4 Ergebnis überprüfen Zuletzt sollten wir das Ergebnis unbedingt anschauen und prüfen, ob alles wie erwartet aussieht head(ds) ## participant factor_A factor_B posture SOA soaNum resp rt ## 1 B006b-08 Level_2 Level_2 uncr 800 800 1 618 ## 2 B006b-08 Level_2 Level_2 uncr 100 100 1 824 ## 3 B006b-08 Level_2 Level_1 uncr 50 50 1 752 ## 4 B006b-08 Level_2 Level_2 uncr 300 300 1 646 ## 5 B006b-08 Level_2 Level_2 crossed 800 800 1 869 ## 6 B006b-08 Level_2 Level_2 crossed 50 50 1 936 str(ds) ## &#39;data.frame&#39;: 32455 obs. of 8 variables: ## $ participant: Factor w/ 21 levels &quot;B006b-08&quot;,&quot;B006b-09&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... ## $ factor_A : Factor w/ 2 levels &quot;Level_1&quot;,&quot;Level_2&quot;: 2 2 2 2 2 2 1 2 1 2 ... ## $ factor_B : Factor w/ 2 levels &quot;Level_1&quot;,&quot;Level_2&quot;: 2 2 1 2 2 2 1 1 2 2 ... ## $ posture : Factor w/ 2 levels &quot;uncr&quot;,&quot;crossed&quot;: 1 1 1 1 2 2 2 1 1 1 ... ## $ SOA : Factor w/ 4 levels &quot;50&quot;,&quot;100&quot;,&quot;300&quot;,..: 4 2 1 3 4 1 1 3 2 4 ... ## $ soaNum : int 800 100 50 300 800 50 50 300 100 800 ... ## $ resp : num 1 1 1 1 1 1 1 1 1 1 ... ## $ rt : num 618 824 752 646 869 ... "],["plausibility-check.html", "Teil 5 Daten auf Plausibilität prüfen", " Teil 5 Daten auf Plausibilität prüfen Zeitplanung: - Anzahl Bedingungen, Trials pro Bedingung etc. direkt zu Anfang prüfen - Plotten der Daten einzelner Vpn direkt zu Anfang programmieren - Anzahl Vpn, Gesamtzahl Trials etc. direkt am Ende der Erhebung prüfen! Bei jedem Auswertungsschritt sollte man prüfen, ob das, was man in den Daten hat, überhaupt Sinn macht. Ganz zu Anfang bietet sich an, erstmal zu schauen: Denkt R, dass ich so viele Versuchspersonen habe wie ich es auch denke? Gibt es pro Vp so viele Zeilen wie ich erwarte (meistens also: so viele Zeilen wie Trials)? Gibt es insgesamt so viele Zeilen/Daten wie ich es erwarte? Außerdem sollte man sich die Daten der einzelnen Vpn mit ggplot plotten (visualisieren) und ansehen. Hierbei sollte man nach ungewöhnlich aussehenden Vpn suchen. Ungewöhnlich heißt z.B.: eine Vp hat in einer oder allen Bedingungen nahe 100% Fehler. Dies weist meistens darauf hin, dass die Antwortknöpfe vertauscht waren. eine Vp hat in einer oder manchen Bedingungen 50% Fehler (oder ein anderes Zufallsniveau, zB 33% bei 3 Wahlmöglichkeiten). Dabei haben alle anderen Vpn nahe 100%. Eine Vp hat ein ganz anderes Antwortmuster (Fehler oder RT) als die anderen Eine Vp war viel schneller oder langsamer als alle anderen … Wie man die die Daten pro Versuchsperson plottet, behandeln wir im nächsten Teil 7. 5.0.1 Anzahl Vpn überprüfen Der “levels”-Befehl zegt an, welche Levels (also Faktorabstufungen) ein Faktor hat. levels( ds$participant ) ## [1] &quot;B006b-08&quot; &quot;B006b-09&quot; &quot;B006b-10&quot; &quot;B006b-11&quot; &quot;B006b-12&quot; &quot;B006b-13&quot; &quot;B006b-14&quot; &quot;B006b-15&quot; &quot;B006b-16&quot; &quot;B006b-17&quot; &quot;B006b-18&quot; &quot;B006b-19&quot; &quot;B006b-20&quot; &quot;B006b-21&quot; &quot;B006b-22&quot; ## [16] &quot;B006b-23&quot; &quot;B006b-24&quot; &quot;B006b-25&quot; &quot;B006b-26&quot; &quot;B006b-27&quot; &quot;B006b-28&quot; Wie viele Vpn habe ich im Datensatz? Dazu kann man schauen, wie lang der eben gezeigte Vektor ist. length( levels( ds$participant ) ) ## [1] 21 Wir können so sehen, dass 21 Vpn enthalten sind. 5.0.2 Anzahl von Trials überprüfen Wenn man weiß, welche Faktoren manipuliert wurden, kann man berechnen, wie viele Zeilen in ds drin sein sollten. Hier war das: 21 Vpn x 2 (posture) x 2 (Faktor A) x 2 (Faktor B) x 8 SOA x 6 Stimulus-Stärken x 4 Wiederholungen = 21 x 1536 = 32.256 Trials Nun können wir diese Anzahl vergelcihen mit der Anzahl an Zeilen in unserem Datensatz. Theoretisch kann man aauch schauen, was einem im Envirnonment angeziegt wird, denn dort steht ebenfalls die Anzahl an Variablen und Zeilen von data.frames. Die in R eingebaute Funktion hierfür ist aber “nrow.” nrow( ds ) ## [1] 32455 Wie man sieht: wir haben mehr. Seltsam. Also mal nach einzelnen Vpn aufschlüsseln. ( sanCheck &lt;- ds %&gt;% group_by( participant ) %&gt;% summarise( n() ) ) ## # A tibble: 21 × 2 ## participant `n()` ## &lt;fct&gt; &lt;int&gt; ## 1 B006b-08 1536 ## 2 B006b-09 1536 ## 3 B006b-10 1536 ## 4 B006b-11 1536 ## 5 B006b-12 1536 ## 6 B006b-13 1536 ## 7 B006b-14 1536 ## 8 B006b-15 1774 ## 9 B006b-16 1536 ## 10 B006b-17 1536 ## # … with 11 more rows Die Funktionen “group_by” und “summarise” sind tidyverse/dplyr-Routinen; sie erstellen nicht einen data.frame, sondern einen “tibble.” Tibbles stellen immer nur einige Zeilen dar. Um alle Zeilen zu sehen, kann man entweder den Tibble als einen dataa.frame anzeigen (auskommentierter Befehl) oder man verwendet die “print” Funktion, mit der man eine beliebge Anzahl an Zeilen darstellen kann und zeigen dabei alle Zeilen an, indem wir die Zeilenanzahl auf “Inf” setzen. #data.frame( sanCheck ) sanCheck %&gt;% print(Inf) ## # A tibble: 21 × 2 ## participant `n()` ## &lt;fct&gt; &lt;int&gt; ## 1 B006b-08 1536 ## 2 B006b-09 1536 ## 3 B006b-10 1536 ## 4 B006b-11 1536 ## 5 B006b-12 1536 ## 6 B006b-13 1536 ## 7 B006b-14 1536 ## 8 B006b-15 1774 ## 9 B006b-16 1536 ## 10 B006b-17 1536 ## # … with 11 more rows Man sieht, dass Vp15 hat mehr Zeilen hat als erwartet; Vp 28 weniger. Hier muss man also schauen, was los war: Muss man Trials entfernen? Muss man Vpn nacherheben? Wir schauen uns Vp15 genauer an. Mit “filter” kann man Zeilen auswählen. Wir wählen alle Zeilen der vp15. ds %&gt;% filter( participant == &quot;B006b-15&quot; ) %&gt;% group_by( factor_A, factor_B, posture ) %&gt;% summarize( n() ) %&gt;% print(Inf) ## `summarise()` has grouped output by &#39;factor_A&#39;, &#39;factor_B&#39;. You can override using the `.groups` argument. ## # A tibble: 8 × 4 ## # Groups: factor_A, factor_B [4] ## factor_A factor_B posture `n()` ## &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt; ## 1 Level_1 Level_1 uncr 192 ## 2 Level_1 Level_1 crossed 247 ## 3 Level_1 Level_2 uncr 192 ## 4 Level_1 Level_2 crossed 254 ## 5 Level_2 Level_1 uncr 192 ## 6 Level_2 Level_1 crossed 254 ## 7 Level_2 Level_2 uncr 192 ## 8 Level_2 Level_2 crossed 251 Wir vergleichen mal mit einer Vp, die die erwartete Anzahl Zeilen hat: ds %&gt;% filter( participant == &quot;B006b-16&quot; ) %&gt;% group_by( factor_A, factor_B, posture ) %&gt;% summarize( n() ) %&gt;% print(Inf) ## `summarise()` has grouped output by &#39;factor_A&#39;, &#39;factor_B&#39;. You can override using the `.groups` argument. ## # A tibble: 8 × 4 ## # Groups: factor_A, factor_B [4] ## factor_A factor_B posture `n()` ## &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt; ## 1 Level_1 Level_1 uncr 192 ## 2 Level_1 Level_1 crossed 192 ## 3 Level_1 Level_2 uncr 192 ## 4 Level_1 Level_2 crossed 192 ## 5 Level_2 Level_1 uncr 192 ## 6 Level_2 Level_1 crossed 192 ## 7 Level_2 Level_2 uncr 192 ## 8 Level_2 Level_2 crossed 192 Es ist also im Vergleich zu sehen, dass Vp16 in jeder Faktor-Kombination genau 192 Trials hat; VP15 dagegen hat bei allen “crossed” Bedingungen zu viele Trials. Und das ist noch nicht mal ganz einheitlich. Irgendetwas ist schiefgegangen. Vielleicht wurde der Versuch zwei mal gestartet und es gab einen ersten Druchgang mit crossed, der dann abgebrochen wurde und diese Trials wurden dennoch eingelesen? Um das herauszufinden, müssen die Unterlagen aus der Testung (Testungsprotokoll/Testungstagebuch) geprüft und ggf. der Einlesevorgang wiederholt werden. Wenn Sie so eine Situation in den Daten entdecken, wäre das ein typischer Fall, bei dem Sie Ihre* Betreuer*in ansprechen sollten, damit der Daten-Einleseprozess wiederholt wird. Alternativ gäbe es die Möglcihkeit, die entsprechenden Zeilen zu identifizieren und zu entfernen. Hier nehmen wir stattdessen an, dass die Vp nicht ausgewertet werden kann, weil wir nicht rekonstruieren können, was da passiert ist. Man kann mit “filter” diese Vp aus ds entfernen: ds &lt;- ds %&gt;% filter( participant != &quot;B006b-15&quot; ) # beachte, diesmal !=, d.h. wir wählen alles, was NICHT Vp15 ist Als nächstes schauen wir uns Vp28 an: ds %&gt;% filter( participant == &quot;B006b-28&quot; ) %&gt;% group_by( factor_A, factor_B, posture ) %&gt;% summarize( n() ) %&gt;% print(Inf) ## `summarise()` has grouped output by &#39;factor_A&#39;, &#39;factor_B&#39;. You can override using the `.groups` argument. ## # A tibble: 8 × 4 ## # Groups: factor_A, factor_B [4] ## factor_A factor_B posture `n()` ## &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt; ## 1 Level_1 Level_1 uncr 192 ## 2 Level_1 Level_1 crossed 180 ## 3 Level_1 Level_2 uncr 192 ## 4 Level_1 Level_2 crossed 183 ## 5 Level_2 Level_1 uncr 192 ## 6 Level_2 Level_1 crossed 184 ## 7 Level_2 Level_2 uncr 192 ## 8 Level_2 Level_2 crossed 182 Wir sehen, dass bei Vp28 bei den “crossed” Bedingungen einige Trials fehlen. Da wir auch hier nicht rekonstruieren können, warum das so ist, entfernen wir auch diese Vp aus den Daten (analog zu oben) ds &lt;- ds %&gt;% filter( participant != &quot;B006b-28&quot; ) Jetzt, wo einige Daten rausgenommen wurden, sollten die noch vorhandenen Levels im Faktor “participant” gelöscht werden, dazu nimmt man die Funktion “droplevels.” ds$participant &lt;- droplevels(ds$participant) Die Einzeldaten sollen unbedingt mit de_r Betreuer_in durchgesehen und diskutiert werden. Dies sollte unmittelbar nach Ende der Erhebung stattfinden, falls Daten nacherhoben werden müssen! "],["clean-data.html", "Teil 6 Daten bereinigen 6.1 Funktion zur Beseitigung von Trials mit zu langen oder zu kurzen RTs 6.2 Anwendung der RT-basierten Bereinigung auf die Daten 6.3 Korrektur der AV Antwort für die Trials mit ungültigen RTs 6.4 Anzahl entfernter Trials berichten 6.5 Datensatz auf gültige Trials beschränken", " Teil 6 Daten bereinigen Man kann Trials ausschließen, deren RT unter oder über einem bestimmten Wert liegt. Die zugrunde liegende Idee ist: bei sehr schnellen Trials hat die Vp schon im Vorhinein die Antwort vorbereitet, und die Antwort hing daher gar nicht vom Stimulus ab bei sehr langsamen Trials hat die Vp nicht richtig aufgepasst oder andere Sachen gemacht, die nicht instruiert waren Wir schließen häufig Trials aus, die eine RT von &lt;100 oder &lt;150 ms haben, sowie eine RT &gt; 2 * s.d. der jeweiligen Bedingung. Warum “jeweilige Bedingung?” Schließt man einfach über das ganze Experiment hinweg Trials aus, dann fallen in systematisch langsameren Bedingungen mehr Trials raus als bei schnelleren Bedingungen. Indem man für jede Bedingung separat die 2 s.d. berechnet, wird dieser Bias umgangen. Ob man überhaupt Trials rausschmeißen soll, ist umstritten. Ein guter Weg ist, die Analyse mit und ohne Rausschmeißen zu machen. Sofern das gleiche rauskommt, braucht man sich keine Gedanken zu machen und es ist egal, welche Version man in die Arbeit nimmt. Mit “gleich” ist dabei gemeint, dass dieselben Bedingungen signifikant werden und alle Schlussfolgerungen bestehen bleiben. Ergeben sich hingegen Unterschiede je nach Rausschmeiß-Strategie, dann sollte man genauer nachforschen, woran das liegt. Es kann bspw. sein, dass es einige sehr wenige Trials mit sehr langer RT gibt, die die s.d. stark erhöhen. Solche Trials sind wahrscheinlich nicht instruktionskonform; dann schmeißt man lieber raus. Findet man keine deutliche Ursache für Unterschiede zwischen verschiedenen Rausschmeiß-Strategien, sollte man mit seine* Betreuer*in sprechen. 6.1 Funktion zur Beseitigung von Trials mit zu langen oder zu kurzen RTs Um den Bearbeitungsprozess zu erleichtern, werden wir eine selbst-geschriebene Funktion verwenden Das Prinzip der Funktion ist wie folgt: Die Funktion nimmt drei Argumente: den Vektor an RTs selbt “upper,” ausgedrückt in s.d., als den Toleranzbereich nach oben “lower,” ausgedrückt in ms als die niedrigste RT, die nicht entfernt wird Werte in dem Vektor, die außerhalb dieser definierten Grenzen liegen, werden auf NA gesetzt Das heißt, wenn man als upper und lower jeweils 2 und 150 wählt, werden alle Werte, die kleiner sind als 150 ms oder weiter vom Mittelwert entfernt sind als 2 s.d. auf NA gesetzt. clean &lt;- function( x, upper, lower ) { replace( x, x &gt; mean( x, na.rm=T ) + upper * sd( x, na.rm=T ) | x &lt; lower, NA ) } 6.1.1 Test der Funktion an einem Toy-Vektor Oft ist es sinnvoll, wenn man eine Funktion schreibt, sie erstmal an einem selbst generierten random Beispiel-Vektor zu testen, bevor man sie auf die echten Daten loslässt. toyvector &lt;- c(200, 220, 231, 242, 414, 512, 313, 222, 110, 713, 240, 337) cat(paste(&quot;Der kleinste Wert in dem Vektor ist&quot;, min(toyvector)), paste(&quot;Der Mittelwert ist&quot;, round(mean(toyvector))), paste(&quot;Die Standardabweichung ist&quot;, round(sd(toyvector))), paste(&quot;Die zweifache Standardabweichung ist&quot;, round(2*sd(toyvector))), paste(&quot;Der obere cutoff ist&quot;, round(mean(toyvector)+2*sd(toyvector))), sep = &quot;\\n&quot;) ## Der kleinste Wert in dem Vektor ist 110 ## Der Mittelwert ist 313 ## Die Standardabweichung ist 165 ## Die zweifache Standardabweichung ist 329 ## Der obere cutoff ist 642 Jetzt wenden wir die Funktion auf den Toy-Vektor an: toyvector_clean &lt;- clean(x=toyvector, upper = 2, lower = 150) data.frame(original=toyvector, cleaned=toyvector_clean) ## original cleaned ## 1 200 200 ## 2 220 220 ## 3 231 231 ## 4 242 242 ## 5 414 414 ## 6 512 512 ## 7 313 313 ## 8 222 222 ## 9 110 NA ## 10 713 NA ## 11 240 240 ## 12 337 337 Wir können hier gut sehen, dass zwei Werte, ein neidriger von 110 ms und ein hoher Wert von 713 ms, der mehr als 2 s.d. über dem Mittelwert liegt, auf NA gesetzt wurden. Die Funktion scheint also gut zu funktionieren. Zum Spaß können wir auch die Funktion nochmal mit radikaleren Einstellungen fahren: clean(x=toyvector, upper = 1.2, lower = 220) ## [1] NA 220 231 242 414 NA 313 222 NA NA 240 337 Wir sehen, dass logischerweise mehr entfernt wird. 6.2 Anwendung der RT-basierten Bereinigung auf die Daten Jetzt können wir die Funktion auf die Daten anwenden. Wie oben beschrieben, wird die Funktion auf jede Bedingung einzeln angewendet. Als Regel nehmen wir aber immer enfernen, wenn RT &gt; 2 s.d. über Mittelwert ODER &lt; 150 ms. Dass der Befehl einzeln für jede Bedingung ausgeführt wird, liegt an dem “group_by”-Befehl, der R sagt, dass es eine Funktion separat für jedes Subset ausführen soll. Als ein Subset bezeichnet man die Daten, die zu einer Kombination der Faktoren in dem “group_by”-Befehl gehören. Den neuen Datensatz nennen wir “dsClean.” dsClean &lt;- ds %&gt;% group_by( participant, factor_A, factor_B, posture, SOA ) %&gt;% mutate(rtClean = clean( rt, 2, 150 )) 6.3 Korrektur der AV Antwort für die Trials mit ungültigen RTs Für die Trials mitt einer zu schnellen oder zu langsamen RT, die wir in dem vorherigen Schritt identifiziert haben, sollten wir auch die Antwort der Vpn entfernen, da diese ebenfalls ungültig sind. Wir legen hierzu eine neue Variable “respClean” an. dsClean$respClean &lt;- dsClean$resp #zunächst gleich wie rep dsClean[ is.na( dsClean$rtClean ), &quot;respClean&quot;] &lt;- NA #Fälle auf NA 6.4 Anzahl entfernter Trials berichten Jede empirische Arbeit sollte klar kommunizieren, wie bei der Bereinigung von Daten vorgegangen worden ist und wie viele Trials dabei entfernt wurden. Die Anzahl der in dem Schritt oben entfernten Daten lässt sich einfach bestimmen. Das ist deshalb möglich, weil die Daten vor der Korrektur noch vorhanden sind. print( paste( &quot;Anzahl Trials gesamt:&quot;, nrow( ds ) ) ) ## [1] &quot;Anzahl Trials gesamt: 29184&quot; print( paste( &quot;Anzahl Trials nach Cleaning: &quot;, nrow(dsClean[ !is.na( dsClean$rtClean ), ] ) ) ) ## [1] &quot;Anzahl Trials nach Cleaning: 27706&quot; print( paste( &quot;Prozent eliminiert:&quot;, round( 100 - nrow(dsClean[ !is.na( dsClean$rtClean ), ] ) / nrow( ds ) * 100, digits = 1 ) ) ) ## [1] &quot;Prozent eliminiert: 5.1&quot; Dieser Wert liegt im Bereich des Erwartbaren. Dadurch, dass oberhalb von 2 s.d. die Daten abgeschnitten wurden, gehen automatsch 4.5 % der Daten verloren. Dass die Prozentzahl daann noch etwas höher ist, liegt daran, dass auch wegen des unteren cutoff-Kriteriums bei 150 ms noch einigen Trials entfernt werden. 6.5 Datensatz auf gültige Trials beschränken Nachdem wir berechnet und berichtet haben, wie viele Trials ausgeschlossen wurden, kann man einen neuen Datensatz machen, der die ungültigen Trials nicht mehr enthält. Das vereinfacht viele Operationen, da R sonst häufig Berechnungen wegen fehlnder Werte nicht durchführt oder in Funktionen explizit gemacht werden muss, wie mit NA umgegangen werden soll. Den neuen Datensatz, der nur noch Daten enthält mit denen wir rechnen wollen, nennen wir einfach “dc” (kurzer Name). dc &lt;- dsClean %&gt;% filter(!is.na(rtClean)) Nun überprüfen wir noch einmal, ob alles stimmt. Wir wollen nicht, dass noch irgendwo NAs sind. sanityCheck &lt;- dc %&gt;% group_by( participant, factor_A, factor_B, posture, SOA ) %&gt;% summarize( N = sum( is.na( resp ) )) ## `summarise()` has grouped output by &#39;participant&#39;, &#39;factor_A&#39;, &#39;factor_B&#39;, &#39;posture&#39;. You can override using the `.groups` argument. sum( sanityCheck$N &gt; 0 ) ## [1] 0 Es ist also alles in Ordnung. Wir können nun mit der Statistik beginnen. "],["basic-plots.html", "Teil 7 Visualisierung der Rohdaten 7.1 Grammar of Graphics 7.2 Visualisierung von Rohdaten pro Vpn 7.3 Visualisierung auf Gruppenebene 7.4 Daten abspeichern", " Teil 7 Visualisierung der Rohdaten Zeitplanung: Abbildungen der Rohdaten sind ein elementarer Teil von Plausibilitäts-Checks. Man sollte sich sofort damit befassen, wenn man die Daten hat 7.1 Grammar of Graphics Das Prinzip der “Grammar of Graphics” ist ausführlich in Hadley Wickhams (Wickham 2009) beschrieben und sprengt den Rahmen dieses Dokuments. Das Wichtigste ist aber: Die Eigenschaften des Plots nennt man “Aesthetics” Beispiele für Aesthetics sind: x- und y-Richtung Farbe von Elementen Form Größe Linienart Beispiele für Eigenschaften der Daten sind typischerweise Ausprägungen von (diskreten oder kontinuierlichen) Variablen, z.B.: Eine kontinueriliche AV Variable wie Reaktionszeit Eine diskrete AV wie Antwort korrekt (ja/nein) Eine diskrete UV, wie ein Bedingungs-Faktor In Datenvisualisierungen (Plots) werden dann Eigenschaften aus der Daten auf bestimmte Eigenschaften des Plots “gemappt” (aesthetic mapping) Grammar of Graphics bedeutet, dass man eine Eigenschaft der Daten auf eine Aesthetic des Plots mappt. Man würde also zum Beispiel die RTs auf die y-Achhse des Plots “mappen” und die Bedingung (sagen wir “crossed” vs. “uncrossed”) als Farbe ausdrücken aber man würde nicht RTs sowohl in y-Position wie auch in dem Farbton ausdrücken. Mapping sollte immer Eins-zu-Eins sein. Es mag natüerlich sein, dass es Situationen gibt, wo man etwas ganz bestimmtes mit einem Doppel-Mapping bezweckt. Es ist nicht “verboten,” das zu tun, aber man sollte es nicht unnötig tun. Man sollte nach Möglichkeit mit dem Mapping konsistent (das heißt, auch über Plots hinweg) sein. D.h., man sollte nicht in Abbildung 1 die Bedingung als Farbe und in Abbildung 2 die Bedingung plötzlich als Form ausdrücken. 7.2 Visualisierung von Rohdaten pro Vpn Das erste, was man tun sollte, ist, dass man sich die Daten nocheinmal gut anschaut und überprüft, ob die Daten plausibel aussehen. Was das bedeutet, haben wir im letzten Teil beschrieben 5. 7.2.1 Visualisierung der AV Antworten Welche Art von Plot man macht, hängt von vielen Parametern des Designs und auch der Stichprobe ab. Da wir ein within-subject Design mit einer relativ kleinen Stichprobe (19 nach Enfernung von 2 Vpn) haben, bietet es sich sich, einzelne Plots für die Personen zu machen. Dies geht mit der “facets”-Funktion von ggplot2. Zunächst berechnen wir aber mal pro Vpn denn Prozentsatz richtiger Antworten. Es bietet sich an, für komplexere Plots einen eigenen Datensatz zu machen. Wir nennen diesen mal “dp” (für data plot). dp &lt;- dc %&gt;% group_by(participant, factor_A, factor_B, posture, SOA) %&gt;% summarise(percCorr=mean(respClean)*100) ## `summarise()` has grouped output by &#39;participant&#39;, &#39;factor_A&#39;, &#39;factor_B&#39;, &#39;posture&#39;. You can override using the `.groups` argument. ggplot(dp, aes(x=SOA, y=percCorr, shape=factor_A, linetype=factor_B, color=posture)) + geom_point() + geom_path(aes(group=interaction(participant, factor_A, factor_B, posture))) + facet_wrap(~participant) Figure 7.1: Anteil korrekter Antworten für jede Vp Das erklärte “Mapping” von Daten auf “Aesthetics” ist (vor allem) in der ersten Zeile des Plot-Befehls in der Funktion “aes” zu sehen. Etwas komplexer ist der Befehl innerhalb von “geom_path” (was Linien macht). Dieser Parameter (group) definiert, welche Beobachtungen zusammen eine Gruppe ergeben. Mit “interaction” sagt man hier, dass die Gruppe die Kombination betrifft aus den genannten Faktoren. Das sind in dem Fall alle außer SOA, denn die Gruppe wird ja über SOA geplottet. Der Plot ist überaus informativ. Es sind sofort Dinge zu sehen, von denen wir bisher gar nichts wussten. Das Wichtigste: VP 16 und 27 sind in den crossed Bedingungen in ihrer Performance verdächtig nahe an 50% (der Ratewahrscheinlichkeit). Da sie aber in den anderen Bedingungen gut performen und auch bei längeren SOA besser sind, kann man aber davon ausgehen, dass sie schon instruktionskonform agiert haben, aber einfach diese Bedingungen sehr schwer fanden (einen besonders starken crossing effect zeigten). Wir lasse die Personen also drin. VP 23 hat Leistungen von fast 0 Prozent. Das ist praktisch unmöglich bei einer Ratewahrscheinlichkeit von 50%. Es würde ja bedeuten, dass die Person exakt entgegengesetzt den Instruktionen gedrückt hat und darin sogar sehr gut war. Es ist sehr wahrscheinlich, dass die Knöpfe falschrum angeschlossen oder hingelegt wurden oder die Instruktion umgekehrt war als sie sollte. Da wir das aber nur aus dem Protokoll entnehmen können, bleibt uns zunächst nichts anderes übrig, als die Person zu entfernen. dc &lt;- dc %&gt;% filter(participant != &quot;B006b-23&quot;) dc$participant &lt;- droplevels(dc$participant) 7.2.2 Visualisierung der AV RTs Als nächstes machen wir einen ähnlichen Plot für die Reaktionszeiten. dp &lt;- dc %&gt;% group_by(participant, factor_A, factor_B, posture, SOA) %&gt;% summarise(meanRT=mean(rtClean)) ## `summarise()` has grouped output by &#39;participant&#39;, &#39;factor_A&#39;, &#39;factor_B&#39;, &#39;posture&#39;. You can override using the `.groups` argument. ggplot(dp, aes(x=SOA, y=meanRT, shape=factor_A, linetype=factor_B, color=posture)) + geom_point() + geom_path(aes(group=interaction(participant, factor_A, factor_B, posture))) + facet_wrap(~participant) Figure 7.2: Reaktionszeiten für jede Vp 7.3 Visualisierung auf Gruppenebene Als nächstes wollen wir die Daten für die Stichprobe plotten. Wir gehen ähnlich vor wie oben, wir lassen aber “participant” sowohl bei dem “group_by” wie auch in den “facet_wrap” weg. 7.3.1 AV Antwort dp &lt;- dc %&gt;% group_by(factor_A, factor_B, posture, SOA) %&gt;% summarise(percCorrect=mean(respClean)*100) ## `summarise()` has grouped output by &#39;factor_A&#39;, &#39;factor_B&#39;, &#39;posture&#39;. You can override using the `.groups` argument. p1 &lt;- ggplot(dp, aes(x=SOA, y=percCorrect, shape=factor_A, linetype=factor_B, color=posture)) + geom_point() + geom_path(aes(group=interaction(factor_A, factor_B, posture))) p1 Figure 7.3: Anteil korrekter Antworten in der Gruppe In dieser Abbildung können wir nun schon einiges über die Ergebnisse lernen, z.B.: dass “crossed” zu mehr Fehlern führt dass bei Faktor_B vor allem “Level_2” zu mehr Fehlern führt dass Faktor_A weniger auszumachen scheint Das sind natürlich erstmal nur deskriptive Beobachtungen. Ob diese Effekte statistisch signifikant sind, wird man erst nach der Statistk sagen können. 7.3.2 AV RT Zuletzt schauen wir uns noch die RT Daten an. dp &lt;- dc %&gt;% group_by(factor_A, factor_B, posture, SOA) %&gt;% summarise(meanRT=mean(rtClean)) ## `summarise()` has grouped output by &#39;factor_A&#39;, &#39;factor_B&#39;, &#39;posture&#39;. You can override using the `.groups` argument. p2 &lt;- ggplot(dp, aes(x=SOA, y=meanRT, shape=factor_A, linetype=factor_B, color=posture)) + geom_point() + geom_path(aes(group=interaction(factor_A, factor_B, posture))) p2 Figure 7.4: Miittlere RTs in der Gruppe Hier kann man deskriptiv sehen, dass in den RT ein inverses Pattern vorliegt wie bei den Antworten. Bei crossed und bei Level_2 von factor_B sind die Antworten langsamer. 7.4 Daten abspeichern Da jetzt die Plausibilitätsprüfung abgeschlossen ist, werden wir die bereinigten Daten erneut abspeichern, so dass sie später leichter wie geladen werden können. save( ds, dc, file = &quot;RTL_beispieldaten_clean.RData&quot;) References "],["basic-stats.html", "Teil 8 Statistische Analysen 8.1 Vorwort zu statistische Analysen 8.2 Die “normale” Anova 8.3 Die repeated measures Anova 8.4 Das linear mixed model (LMM) 8.5 Das generalized linear mixed model (GLMM)", " Teil 8 Statistische Analysen 8.1 Vorwort zu statistische Analysen Bevor wir dezidiert an die Analysen der Antwort- und RT-Daten gehen, hier ein paar allgemeine Informationen vorweg. 8.1.1 Statistische Auswertung von %-Daten (zB korrekt/inkorrekt) Prozentwerte darf man nicht mit einer Anova analysieren. Details dazu finden sich in: Jaeger, T. F. (2008). Categorical data analysis: Away from ANOVAs (transformation or not) and towards logit mixed models. Journal of Memory and Language, 59(4), 434–446. https://doi.org/10.1016/j.jml.2007.11.007 Stattdessen analysiert man sie mit generalized linear mixed models (GLMM). Diese modellieren die Wahrscheinlichkeit, dass ein Trial korrekt oder inkorrekt beantwortet war. Dies passiert auf Ebene eines einzelnen Trials: Statt Prozentwerte zu berechnen, wie viele Trials richtig waren, beantworten GLMM die Frage, wie wahrscheinlich ein gegebener Trial korrekt oder inkorrekt beantwortet wird, gegeben die Faktorausprägungen des jeweiligen Trials. Dabei ändert also ein Faktor die Wahrscheinlichkeit, dass man richtig oder falsch antwortet; dadurch ändert sich natürlich indirekt auch die Prozentzahl richtiger/falscher Trials. Im Rahmen von BSc/MSc-Arbeiten sind v.a. folgende Aspekte wichtig: Prozentwerte nicht mit Anova auswerten, sondern korrekt mit GLMM; wir nutzen dazu das package afex. Dies zieht andere posthoc-Tests nach sich; wir nutzen dazu das package emmeans. Ein wesentlicher Punkt bei GLMM für korr/inkorr Daten ist, dass Veränderungen im Bereich nahe 50% (also Zufallsniveau) sehr viel wahrscheinlicher sind als Veränderungen im Beriech 100% (also perfekte Performance). Das heißt, Veränderungen bei GLMM sind nicht linear im Prozentbereich. Stattdessen sind sie linear auf der Skala, die das GLMM benutzt - dies ist eine logit Skala (also, wer sich damit nicht genauer auseinandersetzen will, der muss das auch nicht - aber es hat Konsequenzen füt die Darstellung, s.u.) Die Nichtlinearität auf der Prozent-Skala wirkt sich darin aus, dass error bars nicht mehr symmetrisch sind. Wie kann das sein? Das GLMM schätzt lineare Effekte auf der logit-Skala. Dort gibt es dann auch ganz normale, lineare s.d. und s.e. Aber wenn man diese dann in Prozentwerte transformiert, ist diese Transformation nicht linear. Wie man das macht, findet sich hier im Skript. 8.1.2 Statistische Auswertung von metrischen Daten mit Anovas und linear mixed models Kontinuierliche oder metrische Daten werden mit einer Anova oder mit einem linear mixed model (LMM) ausgewertet. Dabei ist zu beachten, durch was für eine Art von Design die Daten zustande gekommen sind. Wenn es sich um eine abhängiges, within-subject Design handelt, darf keine “normale”” Anova gerechnet werden, sondern es muss eine repeated-measures Anova (rmAnova) gerechnet werden, die berücksichtigt, dass mehre Daten (z.B. aus unterschiedlichen Bedingungen) von der gleichen Person kommen. In einem solchen Fall besteht auch die Möglichkeit, anstatt einer rmAnova ein LMM zu rechnen. Der Unterschied zwischen einer rmAnova und einem LMM bestehen in den sogenannten random effects, wie random intercepts und random slopes. LMMs “erlauben” Versuchspersonen, ein unterschiedliches Ausgangsniveau (random intercept) und auch unterschiedlich starke Steigungen von Regressionsgeraden zu haben (random slope). An der Terminologie kann man schon erkennen, dass LMMs eher eine Erweiterung von multiplen und gemischten (mehrere Faktoren und kontinuierliche Prädiktoren) Regressionen sind. Insofern ist hier auch der Begriff “Regressionsgerade” etwas weiter zu fassen und eher zu begreifen als ein Koeffizient, der auch die Veränderung zwischen (zwei oder mehr) Faktorabstufungen angibt. LMMs haben gegenüber der rmAnova einige Vorzüge, dazu zählen ein besserer Umgang mit fehlenden Datenpunkten eine hohe Felxibilität in der Modellformulierung, bei der von sehr einfachen bis komplexen Modellen alles innerhalb des gleichen Frameworks berechnet werden kann Ein wichtiger Unterschied und auch Vorzug ist auch, dass ein LMM durch die Verwandtschaft zu Regression ein echtes Modell ist, das heißt, dass es Daten fittet und damit Vorhersagen macht. Unterschiedliche Modelle können ebenfalls gegeneinander getestet werden, um herauszufinden, welches Modell angemessener ist. 8.1.3 RTs als metrische Daten In dem hier behandelten Beispieldatensatz sind Reaktionszeiten (RTs) enthalten. Wir werden sie als metrische Daten fassen und mit einer Anova auswerten. Ob RTs tatsächlich metrische Daten sind oder nicht, ist eine fortlaufende Diskussion und es wurde immer wieder der Einwand gebracht, dass sie transformiert (zum Beispiel log-transformiert) werden sollten, da sie häufig eine schiefe Verteilung haben. Allerdings lässt sich auch zeigen, dass die Transformationen Gefahren bergen und, dass von ihnen eher abgesehen werden sollte. Mehr Informationen findet man in folgendem Artikel: Schramm, P., &amp; Rouder, J. (2019). Are Reaction Time Transformations Really Beneficial? [Preprint]. https://doi.org/10.31234/osf.io/9ksa6 Wir befürworten daher eher eine Auswertung von Reaktionszeiten, wie sie sind, anstatt einer Transformation. 8.1.4 RTs bei korrekten und inkorrekten Trials Es ist in der Literatur nicht ganz einheitlich (und kommt wahrscheinlich auf das Design an), ob Reaktionszeiten auf der Grundlage von allen Trials und ungeachtet der Frage analysiert werden, ob die Antwort in dem Trials korrekt war oder nicht. Es gibt aber gute Gründe zu argumentieren, dass ein Fehler etwas anderes ist als eine korrekte Antwort und, dass die Kombination von RTs von korrekten Antworten und Fehlern zwei Verteilungen vermischen würde. Demnach wäre es aussagekräftiger Fehler und korrekte Antworten separat zu analysieren oder sich die RTs halt nur für die korrekten Antworten anzuschauen. Das ist der Ansatz den wir hier verfolgen. Bei der Erstellung des Datensatzes für die Analysen der RTs werden also immer die inkorrekten Antworten ausgeschlossen. 8.2 Die “normale” Anova Bei dem bestehendem Datensatz bietet sich eine Anova als Auswertung der Verhaltensdaten nicht an, da es sich um ein within-subject Design handelt. Die “normale” Anova wird verwendet, um beispielsweise zu testen, ob sich drei distinkte Gruppen bezüglich einer metrischen Variable unterscheiden. Dass eine VP mehrmals vorkommt, ist nicht erlaubt, da es die Annahme der Unabhängigkeit der Beobachtung verletzt. In diesem Beispiel wird daher analysiert, ob die im Experiment verwendete Körperstellung (welcher Arm oben lag) und das Geschlecht (weiblich/männlich) einen Effekt auf die mittlere Reaktionszeit (gemittelt über alle Bedingungen und SOAs) hat. 8.2.1 Daten vorbereiten Zunächst generieren wir einen passenden Datensatz. Hierzu muss die Information aus dem Datensatz “bsp_demo” mitverwendent und an den andern Datensatz angehängt werden. # mittlere Reaktionszeiten der korrekten Antworten berechnen dAnova &lt;- dc %&gt;% filter(respClean==1) %&gt;% group_by(participant) %&gt;% summarise(n=n(), meanRT=mean(rt)) # Anfangsbuchstaben der Variablennamen klein machen bsp_demo &lt;- bsp_demo %&gt;% rename_all(tolower) # Info aus den demographischen Daten hinzufügen dAnova &lt;- right_join(bsp_demo, dAnova) ## Joining, by = &quot;participant&quot; # Variablen korrekt als Faktoren definieren dAnova$sex &lt;- factor(dAnova$sex) dAnova$arm_top &lt;- factor(dAnova$arm_top) 8.2.2 Der Anova-Befehl und Output Als nächstes rechnen wir die Anova mit dem Befehl “aov_ez” aus dem afex Paket. und bertrachten den Output. meanRTs.anova &lt;- aov_ez(id = &quot;participant&quot;, dv = &quot;meanRT&quot;, data = dAnova, between = c(&quot;sex&quot;, &quot;arm_top&quot;)) ## Contrasts set to contr.sum for the following variables: sex, arm_top Nun betrachten wir den Output der Anova, den sogenannten Anova-Table: meanRTs.anova ## Anova Table (Type 3 tests) ## ## Response: meanRT ## Effect df MSE F ges p.value ## 1 sex 1, 14 17089.40 2.02 .126 .177 ## 2 arm_top 1, 14 17089.40 1.12 .074 .308 ## 3 sex:arm_top 1, 14 17089.40 0.00 &lt;.001 .996 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;+&#39; 0.1 &#39; &#39; 1 Wie man der Anova-Tabelle entnehmen kann, hat keiner der beiden Faktoren Geschlecht und Armstellung einen signifikanten Effekt auf die Reaktionszeit. Auch die Interaktion ist nicht signifikant. Im Normalfall ist die Analyse dann an dieser Stelle beendet, da in der Situation eines nicht signifikanten Tests in der Anova, keine direkten Vergleiche mehr angestellt werden. Für den Fall, dass aber einer der Faktoren oder auch die Interaktion signifikant gewesen wäre, hätte man einen post-hoc Test gerechnet, um herauszufinden, welche der Gruppen sich unterscheiden. 8.2.3 Post-hoc Vergleiche Wir werden aus Demonstrationsgründen hier einfach den post-hoc Test trotzdem rechnen. Dazu verwenden wir den Befehl “emmeans” aus dem gleichnamigen Paket. Das Paket emmeans ist sehr mächtig, es ist aber auch nicht ganz einfach in der Handhabung. Es lohnt sich für einen vertieften Einstig entweder die Vignette des Pakets unter https://cran.r-project.org/web/packages/emmeans/vignettes/comparisons.html oder diesen Blog https://aosmith.rbind.io/2019/03/25/getting-started-with-emmeans/ zu lesen. Zunächst schauen wir uns den Haupteffekt von “sex” an: emmeans(meanRTs.anova, pairwise ~ sex, adjust = &quot;fdr&quot;) ## NOTE: Results may be misleading due to involvement in interactions ## $emmeans ## sex emmean SE df lower.CL upper.CL ## female 765 39.6 14 680 850 ## male 856 49.9 14 749 963 ## ## Results are averaged over the levels of: arm_top ## Confidence level used: 0.95 ## ## $contrasts ## contrast estimate SE df t.ratio p.value ## female - male -90.6 63.7 14 -1.422 0.1768 ## ## Results are averaged over the levels of: arm_top ## P value adjustment: fdr method for 1 tests Nun schauen wir uns die Interaktion von sex und arm_top an. Wenn, wie häufig bei Anova mehr als zwei levels bei einem Faktor vorkommen, dann sollte man unbedingt p-Werte von post-hoc Tests korrigieren. Als Methoden bieten sich Bonferroni oder False Discovery Rate (fdr) an. Diese Methoden lassen sich bei emmeans einfach anwenden indem sie bei dem “adjust”-Parameter definiert werden. emmeans(meanRTs.anova, pairwise ~ sex:arm_top, adjust = &quot;fdr&quot;) ## $emmeans ## sex arm_top emmean SE df lower.CL upper.CL ## female left arm on top 799 53.4 14 684 913 ## male left arm on top 890 75.5 14 728 1051 ## female right arm on top 732 58.5 14 606 857 ## male right arm on top 822 65.4 14 682 962 ## ## Confidence level used: 0.95 ## ## $contrasts ## contrast estimate SE df t.ratio p.value ## female left arm on top - male left arm on top -90.9 92.4 14 -0.984 0.6108 ## female left arm on top - female right arm on top 67.0 79.2 14 0.847 0.6108 ## female left arm on top - male right arm on top -23.3 84.4 14 -0.276 0.7868 ## male left arm on top - female right arm on top 158.0 95.5 14 1.655 0.6108 ## male left arm on top - male right arm on top 67.7 99.8 14 0.678 0.6108 ## female right arm on top - male right arm on top -90.3 87.7 14 -1.030 0.6108 ## ## P value adjustment: fdr method for 6 tests #alternativer code: #emm.sex_armtop &lt;- emmeans(meanRTs.anova, ~ sex:arm_top) #contrast(emm.sex_armtop, method = &quot;pairwise&quot;, adjust = &quot;fdr&quot;) 8.3 Die repeated measures Anova Im Gegensatz zu der “normalen” Anova, ist eine repeated measures anova (rm-Anova) dazu geeignet, um Daten auszuwerten, die mittels eines within-subject-Designs (das heißt mit mehren Messpunkten pro Person; wie bei dem gegebenen Datensatz) gewonnen wurden. Im Folgenden wird ein Datensatz erstellt, bei dem zunächst die RTs über die SOAs innerhalb jeder Person gemittelt werden. Dann wird der Einfluss von factor_A (gleich/ungleich) und factor_B (gleich/ungleich) und posture (crossed/uncrossed) auf die RTs analysiert. 8.3.1 Daten vorbereiten Da auch rm-Anova pro Person und Faktorkombination nur einen Wert erwartet, berechnen wir die mittlere Reaktionszeiten der entsprechenden Faktorkombinationen für die Vpn. drmAnova &lt;- dc %&gt;% filter(respClean==1) %&gt;% group_by(participant, factor_A, factor_B, posture) %&gt;% summarise(meanRT=mean(rtClean)) ## `summarise()` has grouped output by &#39;participant&#39;, &#39;factor_A&#39;, &#39;factor_B&#39;. You can override using the `.groups` argument. Bei rm-Anova sollte man auch überprüfen, dass es auch keine missing values gibt, weil das problematisch ist. Das heßt, es sollte für jede Faktorkombi pro Person genau ein Wert da sein. Das kann man mit dem Befehl “table” prüfen. with(drmAnova, table(participant, factor_A, factor_B, posture)) ## , , factor_B = Level_1, posture = uncr ## ## factor_A ## participant Level_1 Level_2 ## B006b-08 1 1 ## B006b-09 1 1 ## B006b-10 1 1 ## B006b-11 1 1 ## B006b-12 1 1 ## B006b-13 1 1 ## B006b-14 1 1 ## B006b-16 1 1 ## B006b-17 1 1 ## B006b-18 1 1 ## B006b-19 1 1 ## B006b-20 1 1 ## B006b-21 1 1 ## B006b-22 1 1 ## B006b-24 1 1 ## B006b-25 1 1 ## B006b-26 1 1 ## B006b-27 1 1 ## ## , , factor_B = Level_2, posture = uncr ## ## factor_A ## participant Level_1 Level_2 ## B006b-08 1 1 ## B006b-09 1 1 ## B006b-10 1 1 ## B006b-11 1 1 ## B006b-12 1 1 ## B006b-13 1 1 ## B006b-14 1 1 ## B006b-16 1 1 ## B006b-17 1 1 ## B006b-18 1 1 ## B006b-19 1 1 ## B006b-20 1 1 ## B006b-21 1 1 ## B006b-22 1 1 ## B006b-24 1 1 ## B006b-25 1 1 ## B006b-26 1 1 ## B006b-27 1 1 ## ## , , factor_B = Level_1, posture = crossed ## ## factor_A ## participant Level_1 Level_2 ## B006b-08 1 1 ## B006b-09 1 1 ## B006b-10 1 1 ## B006b-11 1 1 ## B006b-12 1 1 ## B006b-13 1 1 ## B006b-14 1 1 ## B006b-16 1 1 ## B006b-17 1 1 ## B006b-18 1 1 ## B006b-19 1 1 ## B006b-20 1 1 ## B006b-21 1 1 ## B006b-22 1 1 ## B006b-24 1 1 ## B006b-25 1 1 ## B006b-26 1 1 ## B006b-27 1 1 ## ## , , factor_B = Level_2, posture = crossed ## ## factor_A ## participant Level_1 Level_2 ## B006b-08 1 1 ## B006b-09 1 1 ## B006b-10 1 1 ## B006b-11 1 1 ## B006b-12 1 1 ## B006b-13 1 1 ## B006b-14 1 1 ## B006b-16 1 1 ## B006b-17 1 1 ## B006b-18 1 1 ## B006b-19 1 1 ## B006b-20 1 1 ## B006b-21 1 1 ## B006b-22 1 1 ## B006b-24 1 1 ## B006b-25 1 1 ## B006b-26 1 1 ## B006b-27 1 1 Das sieht alles richtig aus. 8.3.2 Anova-Befehl und Output Der wichtige Unterschied zu vorher ist der “within” Faktor. meanRTs.rmanova &lt;- aov_ez(id = &quot;participant&quot;, dv = &quot;meanRT&quot;, data = drmAnova, within = c(&quot;factor_A&quot;, &quot;factor_B&quot;, &quot;posture&quot;)) #Ergebnisse in der Konsole anzeigen lassen meanRTs.rmanova ## Anova Table (Type 3 tests) ## ## Response: meanRT ## Effect df MSE F ges p.value ## 1 factor_A 1, 17 1209.01 0.05 &lt;.001 .826 ## 2 factor_B 1, 17 10669.09 5.74 * .019 .028 ## 3 posture 1, 17 31739.73 30.85 *** .234 &lt;.001 ## 4 factor_A:factor_B 1, 17 1093.64 1.32 &lt;.001 .266 ## 5 factor_A:posture 1, 17 561.62 0.00 &lt;.001 .973 ## 6 factor_B:posture 1, 17 3091.67 33.92 *** .032 &lt;.001 ## 7 factor_A:factor_B:posture 1, 17 563.88 0.59 &lt;.001 .454 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;+&#39; 0.1 &#39; &#39; 1 Wie man in der Anova Tabelle sieht, sind “factor_B,” “posture” und auch die Interaktion aus “faktor_B” und “Posture” signifikant. Die Lage ist also schon ein Stück komplexer geartet. 8.3.3 Plots von estimated marginal means zum Nachvollziehen der Interaktion Im Folgenden wird ein graphischer Eindruck über die Werte vermittelt und die Interaktion heruntergebrochen. Das Paket emmeans kann uns hierbei wieder weiterhelfen. Es ist nämlich nicht nur dazu da, um post-hoc Tests zu rechnen, sondern auch, um estimated marginal means (EMM) zu berechnen. EMM sind vom Modell geschätzte Werte übder die Randhäufigkeiten (also für Faktoren und Faktorkombinationen). Beim Herunterbrechen einer Interaktion mittels EMM fängt man normalerweise mit der höchsten signifikanten Kombination an. In diesem Fall ist das die aus factor_B und posture. emm.factorBXposture &lt;- emmeans(meanRTs.rmanova, ~factor_B:posture) print(emm.factorBXposture) ## factor_B posture emmean SE df lower.CL upper.CL ## Level_1 uncr 729 34.2 17 657 801 ## Level_2 uncr 716 23.4 17 667 766 ## Level_1 crossed 840 40.2 17 755 925 ## Level_2 crossed 935 42.6 17 846 1025 ## ## Results are averaged over the levels of: factor_A ## Confidence level used: 0.95 Die EMM kann man auch plotten, um sich einen besseren Überblick zu veschaffen: ggplot(as.data.frame(emm.factorBXposture), aes(x=factor_B, y=emmean, color=posture)) + geom_point(position = position_dodge(width=0.5), size=3) + geom_errorbar(aes(ymin=lower.CL, ymax=upper.CL), position = position_dodge(width=0.5), width=0.2) + ylab(label = &quot;reaction time [ms]&quot;) In der Abbildung kann man sehen, dass die Reaktionszeiten insgesamt bei der “crossed” posture höher sind, was den in dem Anova-Table sichtbaren Haupteffekt von “posture” reflektiert. Zudem kann man sehen, dass die Reaktionszeit bei “Level_2” für den factor_B bei “crossed” erhöht ist und bei uncrossed nicht (hier ist die RT sogar etwas niedriger). Dieses Verhältnis spiegelt die signifikante Interaktion wieder. 8.3.4 Post-hoc paarweiser Vergleich für die factor_B x posture Interaktion Die Interaktion wird nun noch mit post-hoc Tests runtergebrochen, um zu prüfen, welche der Unterschiede signifikant sind. contrast(emm.factorBXposture, method = &quot;pairwise&quot;, adjust = &quot;fdr&quot;) ## contrast estimate SE df t.ratio p.value ## Level_1 uncr - Level_2 uncr 12.7 18.7 17 0.682 0.5042 ## Level_1 uncr - Level_1 crossed -111.0 24.6 17 -4.504 0.0005 ## Level_1 uncr - Level_2 crossed -206.2 38.6 17 -5.341 0.0002 ## Level_2 uncr - Level_1 crossed -123.7 29.4 17 -4.204 0.0007 ## Level_2 uncr - Level_2 crossed -218.9 36.4 17 -6.006 0.0001 ## Level_1 crossed - Level_2 crossed -95.2 20.4 17 -4.670 0.0004 ## ## Results are averaged over the levels of: factor_A ## P value adjustment: fdr method for 6 tests Wie man in der Tabelle sehen kann, sind also alle Vergleiche zwischen den Stufen signifikant mit Ausnahme des Vergleiches zwischen “Level_1”” und “Level_2”” bei “uncrossed.” Somit lassen sich sowohl der Haupteffekt wie auch die Interaktion mittels der post-hoc Vergleiche nachvollziehen. 8.4 Das linear mixed model (LMM) Zuletzt wird noch das LMM vorgestellt, das sich auch gut eignet, um metrische repeated measures Daten zu analysieren. Trotz der mathematischen Unterschiede zu einer rm-Anova, lässt sich der Output recht ähnlich interpretieren. Ein Unterschied ist, dass LMMs in der Regel auf allen Daten beruhen und man nicht vorher, wie oben, über bestimmte Levels aggregiert (Mittelwerte gebildet) werden müssen. Hier werden wir also einfach alle SOAs und alle Wiederholungen der Trials des gleichen Typs “reinfüttern.” In der Anwendung ist ein Unterschied, dass das LMM mit dem afex-Befehl “mixed” angesprochen wird und, dass man die Faktorstruktur über eine Formel angibt. Sie können in dem Befehl unten (im nächsten Abschnitt) sehen, dass einige Parameter gesetzt werden, die Sie noch nicht kennen. Das eine ist der sogenannte random effect. Hier haben wir ihn definiert als “(1|participant).” Diese Art von random effect nennt man einen random intercept für den Faktor Versuchsperson. Kurz ausgedrückt bedeutet das, dass das Modell berücksichtigt, dass die Versuchspersonen einen eigenen Ausgangspunkt haben. Das ist die minimale Annahme, die wir für ein LMM machen können. Die korrekte Formulierung der random effects ist eine der größeren Kontroversen zu dieser Art von Modellen. Diese Kontroverse ist z.B. in folgendem Artikel behandelt: Barr, D. J., Levy, R., Scheepers, C., &amp; Tily, H. J. (2013). Random effects structure for confirmatory hypothesis testing: Keep it maximal. Journal of Memory and Language, 68(3), 255–278. https://doi.org/10.1016/j.jml.2012.11.001 Der andere Parameter, der für Sie neu sein dürfte ist “method.” Hier wird eine von mehreren zur Verfügugn stehenden Methoden für die Schätzung der Freiheitsgrade definiert. Es gibt: Kenward-Rogers “KR”: langsam Satterthwaite “S”: etwas schneller Asymptotic: das schnellste, schätzt df auf Inf Wir nehmen hier “S.” 8.4.1 Daten vorbereiten Wie oben beschreiben, wird nicht aggregiert, wir sortieren aber wieder die falschen Antworten raus. dLMM &lt;- dc %&gt;% filter(respClean==1) 8.4.2 Mixed-Befehl und Output meanRTs.lmm &lt;- mixed(rtClean~factor_A*factor_B*posture + (1|participant), data = dLMM, method = &quot;S&quot;) ## Contrasts set to contr.sum for the following variables: factor_A, factor_B, posture, participant ## Fitting one lmer() model. [DONE] ## Calculating p-values. [DONE] Den Output schaut man wie zuvor an: meanRTs.lmm ## Mixed Model Anova Table (Type 3 tests, S-method) ## ## Model: rtClean ~ factor_A * factor_B * posture + (1 | participant) ## Data: dLMM ## Effect df F p.value ## 1 factor_A 1, 24290.02 0.58 .444 ## 2 factor_B 1, 24290.20 191.66 *** &lt;.001 ## 3 posture 1, 24290.37 2827.75 *** &lt;.001 ## 4 factor_A:factor_B 1, 24290.01 5.52 * .019 ## 5 factor_A:posture 1, 24290.01 0.03 .858 ## 6 factor_B:posture 1, 24290.07 330.74 *** &lt;.001 ## 7 factor_A:factor_B:posture 1, 24290.01 1.93 .165 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;+&#39; 0.1 &#39; &#39; 1 Auch beim LMM bekommt man einen Anova-Table. Der Output lässt sich genau so interpretieren, wie bei der Anova. Auch hier bekommen wir gezeigt, dass es einen signifikanten Haupteffekt von “factor_B” und “posture” gibt. Und eine Interaktion aus “factor_B” und “posture,” wie bei der rmAnova. Einziger Unterschied ist hier, dass das LMM auch noch eine (gerade so) signifikante Interaktion zwischen “factor_A” und “factor_B” anzeigt. Der folgende Plot soll aufzeigen, was es mit den Interaktionen auf sich hat. 8.4.3 Plots von EMM für die factor_B x posture Interaktion Wir beginnen zunächst wieder mit der Interaktiion aus “factor_B” und “posture,” die wir schon aus der rm-Anova kennen. Bevor wir die emms berechnen, sagen wir R noch, welche Methoden verwendet werden sollen, um die Freiheitsgrade zu schätzen. Das Paket emmeans kann mehrere verwenden, die sich mathematisch unterscheiden und teilweise sehr unterschiedlich lange Berechnungsdauern haben (ähnlich wie oben). Wir wählen die Methode, die “asymptotic” heißt. #options für df schätzung bei post hoc tests emm_options(lmer.df = &quot;asymptotic&quot;) # options: &#39;satterthwaite&#39;, &#39;kenward-roger&#39;, &#39;asymptotic&#39; #Wenn Sie &#39;satterthwaite&#39; verwenden wollen, müssen Sie das Limit an Tests hochsetzen #emm_options(lmerTest.limit = 30000) emm.lmm.factorBXPosture &lt;- emmeans(meanRTs.lmm, ~factor_B:posture) ## NOTE: Results may be misleading due to involvement in interactions ggplot(as.data.frame(emm.lmm.factorBXPosture), aes(x=factor_B, y=emmean, color=posture)) + geom_point(position = position_dodge(width=0.5), size=3) + geom_errorbar(aes(ymin=asymp.LCL, ymax=asymp.UCL), position = position_dodge(width=0.5), width=0.2) + ylab(label = &quot;reaction time [ms]&quot;) Das sieht im Wesentlichen aus wie oben bei der rmAnova. 8.4.4 Post-hoc paarweiser Vergleich für die factor_B x posture Interaktion Das Vorgehen ist identisch wie bei der rm-Anova: contrast(emm.lmm.factorBXPosture, method = &quot;pairwise&quot;, adjust = &quot;fdr&quot;) ## contrast estimate SE df z.ratio p.value ## Level_1 uncr - Level_2 uncr 13.6 4.33 Inf 3.138 0.0017 ## Level_1 uncr - Level_1 crossed -109.9 4.37 Inf -25.126 &lt;.0001 ## Level_1 uncr - Level_2 crossed -210.3 4.49 Inf -46.884 &lt;.0001 ## Level_2 uncr - Level_1 crossed -123.5 4.39 Inf -28.142 &lt;.0001 ## Level_2 uncr - Level_2 crossed -223.9 4.50 Inf -49.779 &lt;.0001 ## Level_1 crossed - Level_2 crossed -100.4 4.53 Inf -22.152 &lt;.0001 ## ## Results are averaged over the levels of: factor_A ## Degrees-of-freedom method: asymptotic ## P value adjustment: fdr method for 6 tests Auch die Vergleiche fallen von der Interpretation her ähnlich aus wie oben bei der rmAnova. Es fällt aber ins Augem, dass auch der erste Vergleich zwischen Level_1 und Level_2 bei uncrossed signifikant wird. Man sieht auch, dass wir uns auch bei anderen Vergleichen durch das LMM etwas “sicherer” sein können, denn die p-Werte sind niedriger. Hier drückt sich die etwas höhere Power des LMM aus, das alle Trials mit einbezieht (die rmAnova war ja auf der Basis von Mittelwerten berechnet worden). 8.4.5 Plots von EMM für die factor_B x factor_A Interaktion Im Folgenden soll noch einmal die Abhängigkeit von “factor_A”von “factor_B” verdeutlicht werden (das war die “gerade so” signifikante Interaktion, die die rmAnova nicht zeigte). emm.lmm.factorAXfactorB &lt;- emmeans(meanRTs.lmm, ~factor_A:factor_B) ## NOTE: Results may be misleading due to involvement in interactions ggplot(as.data.frame(emm.lmm.factorAXfactorB), aes(x=factor_B, y=emmean, shape=factor_A)) + geom_point(position = position_dodge(width=0.5), size=3) + geom_errorbar(aes(ymin=asymp.LCL, ymax=asymp.UCL), position = position_dodge(width=0.5), width=0.2) + ylab(label = &quot;reaction time [ms]&quot;) Man sieht ein bisschen, dass sich die Stufen von “factor_A” über “factor_B” in leicht unterschiedliche Richtungen bewegen, also eine leichte Interaktion vorhanden ist. Auch hier kann man wieder einen post-hoc-Test rechnen. 8.4.6 Post-hoc paarweiser Vergleich für die factor_B x factor_A Interaktion Das Vorgehen ist wiederum analog zu oben. contrast(emm.lmm.factorAXfactorB, method = &quot;pairwise&quot;, adjust = &quot;fdr&quot;) ## contrast estimate SE df z.ratio p.value ## Level_1 Level_1 - Level_2 Level_1 4.97 4.37 Inf 1.136 0.2559 ## Level_1 Level_1 - Level_1 Level_2 -36.05 4.43 Inf -8.144 &lt;.0001 ## Level_1 Level_1 - Level_2 Level_2 -45.81 4.43 Inf -10.332 &lt;.0001 ## Level_2 Level_1 - Level_1 Level_2 -41.02 4.43 Inf -9.252 &lt;.0001 ## Level_2 Level_1 - Level_2 Level_2 -50.78 4.44 Inf -11.435 &lt;.0001 ## Level_1 Level_2 - Level_2 Level_2 -9.76 4.49 Inf -2.172 0.0358 ## ## Results are averaged over the levels of: posture ## Degrees-of-freedom method: asymptotic ## P value adjustment: fdr method for 6 tests 8.5 Das generalized linear mixed model (GLMM) Das GLMM ist in der Anwendung sehr ähnliich wie das LMM, ist aber, wie oben beschrieben, dazu da, um diskrete (z.B. ja/nein)-Antworten auszuwerten. In der Anwendung ist der wesentliche Unterschied im Code zu dem LMM, dass wir hier einen Parameter setzen: family=“binomial.” Das sagt dem “mixed”-Befehl, dass es sich um ein GLMM handelt mit einer binären AV. Zusätzlich müssen wir auch eine Methode wählen, um die p-Werte zu schäzten. Hier hat man die Wahl zwischen: Parametric Bootstrap (“PB”): das ist eine sichere Wahl, die eine gute Schätzung liefert für unterschiedlichste Modelle bzw. Faktorstrukturen. Der Nachteil ist, dass die Schätzung je nach Modell sehr lange dauern kann (tw. mehrere Stunden). Likelihood-Ratio-Test (“LRT”): Dieses Verfahren geht deutlich schneller, aber die Schätzungen können ungenau werden, wenn eine einfache random-effect-Struktur gewählt wurde (siehe ?mixed für Informationen) dazu. Da GLMMs besonders bei “PB” eine deutlich längere Berechnungsdauer als andere Modelle haben, lohnt es sich mehrere Prozessoren gleichzeitig zu nuten und es macht unter diesen Umständen Sinn, die Ergebnisse abzuspeichern, damit man das Modell nicht erneut berechnen musss, wenn man die R-Session neu startet. 8.5.1 Der GLMM-Befehl und Output Wir zeigen hier ein Beispiel, wie das fitten mit mehreren Prozessorkernen und das abspeichern der Daten geht: library( parallel ) #für Multicore-Prozesse nc &lt;- detectCores() - 1 # Anzahl an Prozessoren minus 1 # Um zu sehen, was der Prozess macht, wird Output in eine Textdatei geschrieben. cl &lt;- makeCluster(rep(&quot;localhost&quot;, nc), outfile = &quot;cl1.log.txt&quot;) startTime &lt;- Sys.time() #Anfangszeit, um zu sehen, wie lange das Modell läuft resp.glmm &lt;- mixed( respClean ~ posture * factor_A * factor_B + ( 1 | participant ), data = dc, family = &quot;binomial&quot;, method = &quot;PB&quot;, args_test=list(nsim = 500), check_contrasts = TRUE, cl = cl ) # family = endTime &lt;- Sys.time() #Endzeit für den Vergleich mit der Anfangszeit ( endTime- startTime ) #Modellergebnisse abspeichern save(resp.glmm, file = &quot;resp_glmm.RData&quot;) Hier wählen wir noch einen Ansatz mit der “LRT”-Methode: resp.glmm &lt;- mixed( respClean ~ posture * factor_A * factor_B + ( 1 | participant ), data = dc, family = &quot;binomial&quot;, method = &quot;LRT&quot;, check_contrasts = TRUE ) ## Contrasts set to contr.sum for the following variables: posture, factor_A, factor_B, participant ## Fitting 8 (g)lmer() models: ## [........] Als nächstes schauen wir den Output (Anova Table) an: resp.glmm ## Mixed Model Anova Table (Type 3 tests, LRT-method) ## ## Model: respClean ~ posture * factor_A * factor_B + (1 | participant) ## Data: dc ## Df full model: 9 ## Effect df Chisq p.value ## 1 posture 1 649.80 *** &lt;.001 ## 2 factor_A 1 1.63 .201 ## 3 factor_B 1 150.84 *** &lt;.001 ## 4 posture:factor_A 1 0.02 .876 ## 5 posture:factor_B 1 14.56 *** &lt;.001 ## 6 factor_A:factor_B 1 0.31 .576 ## 7 posture:factor_A:factor_B 1 1.94 .164 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;+&#39; 0.1 &#39; &#39; 1 So ähnlich wie bei den RTs sehen wir auch hier einen Haupteffekt von “posture” und “factor_B,” sowie eine signifikante Interaktion “posture:factor_B.” 8.5.2 EMM für das GLMM Auch hier kann man diese Interaktion am besten mit einem Plot und post-hoc Tests nachvollziehen. Grundsätzlich geht das ähnlich wie bei dem LMM, aber es gibt einen Unterschied: das GLMM hat die Daten auf eine logit-Skala transformiert, die etwas kompliziert ist nachzuvollziehen. Wir können aber in “emmeans” definieren, dass wir die Daten wieder auf die Einheit der Antwort (also zwischen 0 und 1) zurücktransformieren wollen. Als Resultat bekommen wird Werte, die Schätzungen für den Anteil korrekter Antworten (den Wert 1 in unserer Variable) in den Faktorkombinationen sind. Dazu defnieren wir in “emmeans” einfach das Argument: type=“response.” emm.resp.glmm &lt;- emmeans(resp.glmm, ~posture:factor_B, type = &quot;response&quot;) ## NOTE: Results may be misleading due to involvement in interactions print(emm.resp.glmm) ## posture factor_B prob SE df asymp.LCL asymp.UCL ## uncr Level_1 0.985 0.00389 Inf 0.975 0.991 ## crossed Level_1 0.952 0.01168 Inf 0.923 0.971 ## uncr Level_2 0.976 0.00618 Inf 0.960 0.985 ## crossed Level_2 0.884 0.02609 Inf 0.822 0.926 ## ## Results are averaged over the levels of: factor_A ## Confidence level used: 0.95 ## Intervals are back-transformed from the logit scale 8.5.3 Plot der EMM aus dem GLMM ggplot(as.data.frame(emm.resp.glmm), aes(x=factor_B, y=prob, color=posture)) + geom_point(position = position_dodge(width=0.5), size=3) + geom_errorbar(aes(ymin=asymp.LCL, ymax=asymp.UCL), position = position_dodge(width=0.5), width=0.2) + ylab(label = &quot;proportion of correct response&quot;) Figure 8.1: EMM für die Interaktion im GLMM 8.5.4 Post-hoc paarweise Vergleiche für die Interaktion im GLMM Für die paarweisen Vergleiche kann man identisch vorgehen wie zuvor: contrast(emm.resp.glmm, method = &quot;pairwise&quot;, adjust = &quot;fdr&quot;) ## contrast odds.ratio SE df null z.ratio p.value ## uncr Level_1 / crossed Level_1 3.310 0.3135 Inf 1 12.636 &lt;.0001 ## uncr Level_1 / uncr Level_2 1.646 0.1706 Inf 1 4.808 &lt;.0001 ## uncr Level_1 / crossed Level_2 8.670 0.7727 Inf 1 24.236 &lt;.0001 ## crossed Level_1 / uncr Level_2 0.497 0.0407 Inf 1 -8.533 &lt;.0001 ## crossed Level_1 / crossed Level_2 2.619 0.1618 Inf 1 15.591 &lt;.0001 ## uncr Level_2 / crossed Level_2 5.268 0.3961 Inf 1 22.096 &lt;.0001 ## ## Results are averaged over the levels of: factor_A ## P value adjustment: fdr method for 6 tests ## Tests are performed on the log odds ratio scale Das einzige, was hier neu ist, ist die Teststatistik “odds.ratio.” Eine odds ratio ist das Verhältnis von zwei Wahrscheinlichkeiten. Eine korrekte Antwort ist also bei uncr Level_1 3.3 mal wahrscheinlicher als bei crossed Level_1 und das ist statistisch signifikant. "],["bis-score.html", "Teil 9 Verknüpfung von Antworten und Reaktionszeiten in ein Maß 9.1 Speed-Accuracy-Tradeoff 9.2 Der BIS-Score 9.3 Analyse des BIS-Scores 9.4 Visualisierung der Ergbenisse des Modells 9.5 Post-hoc Vergleich zum BIS-Score", " Teil 9 Verknüpfung von Antworten und Reaktionszeiten in ein Maß 9.1 Speed-Accuracy-Tradeoff Der Zusammenhang von den in der kognitiven Psychologie typischen Verhaltensmaßen Reaktionszeit und (Korrektheit von) Antworten ist ein seit vielen Jahrzehnten viel diskutiertes Thema. Grundsätzlich ist die Idee, dass schwierige Kognitive Prozesse erstens länger dauern und zweitens zu mehr Fehlern führen. Demnach ist zu erwarten, dass eine höhere Anzahl an Fehlern auch mit längeren RTs einhergeht. Dieser Gedanke übersieht allerdings, dass sich Reaktionszeit und Fehlerhäufigkeit auch (teilweise mit einer strategischen Komponente) gegeneinander austauschen lassen. Eine Person könnte zum Beispiel sich mehr Zeit lassen beim Antworten mit dem Ziel möglichst wenig Fehler zu machen. (Wenn das aufgeht) hätte sie demnach wenig Fehler, aber lange RT. Eine andere Person könnte riskanter vorgehen und versuchen möglichst schnell zu antworten und dabei in Kauf zu nehmen, dass sie halt mehr Fehler macht. Sie hätte demnach mehr Fehler, aber dafür eine kurze RT. Dieses Phänomen, dass sozusagen Genauigkeit (wenig Fehler machen) gegen Geschwindigkeit “eingekauft” werden kann, ist in der Psychologie als ein Speed-Accuracy-Tradeoff bekannt und es ist vielfach belegt, dass Personen das tun und damit sozusagen die Idee, dass Genauigkeit und RT invers zusammenhängen, ein Stück weit untergraben. 9.2 Der BIS-Score Es gibt unterschiedliche Ansätze, Reaktionszeiten und Genauigkeiten (Korrektheit der Antworten) miteinander zu verknüpfen und so zu berücksichtigen, dass VPn (in unterschiedlichem Stil) einen Speed-Accuracy-Tradeoff machen. Eine neue Methode ist die Berechnung des sogenannten BIS-Scores. Wie der BIS-Score berechnet wird und auch, wie er sich von anderen Methoden mit einem ähnlichen Ziel abhebt, ist in dem dazugehörigen Paper nachzulesen: Liesefeld, H. R., &amp; Janczyk, M. (2019). Combining speed and accuracy to control for speed-accuracy trade-offs(?). Behavior Research Methods, 51(1), 40–60. https://doi.org/10.3758/s13428-018-1076-x Im Folgenden wird der BIS-Score für die Beispieldaten berechnet. Dazu wird zunächst ein Datensatz gebraucht, der folgende Info enthält: die Anzahl korrekt beantwortete Trials die Anzahl aller Trials die mittleren RTs für korrekt beantwortete Trials die mittleren RTs für alle Trials Wir nennen den Datensatz mal “dc.m.” dc.m &lt;- dc %&gt;% group_by( participant, posture, factor_A, factor_B ) %&gt;% summarise( sdRtCorr = sd( rtClean[ respClean == 1 ] ), rtCorr = mean( rtClean[ respClean == 1 ] ), rtAll = mean( rtClean ), corrCount = sum( respClean == 1 ), totalCount = n(), pc = corrCount / totalCount *100 ) ## `summarise()` has grouped output by &#39;participant&#39;, &#39;posture&#39;, &#39;factor_A&#39;. You can override using the `.groups` argument. Jetzt können wir den BIS-Score berechnen. # BIS dc.m &lt;- dc.m %&gt;% group_by( participant ) %&gt;% mutate( zRt = ( rtCorr - mean( rtCorr ) ) / sqrt( sum ( ( rtCorr - mean( rtCorr ) ) ^ 2 ) / n() ), zPc = ( pc - mean( pc ) ) / sqrt( sum ( ( pc - mean( pc ) ) ^ 2 ) / n() ), bis = zPc - zRt ) Dabei sollte beachtet werden, dass der BIS-Score innerhalb der Versuchsperson berechnet ist und damit normalisiert wird. Das heißt, der BIS-Score drückt die Unterschiede zwischen Bedingungen aus und nicht die UNtersdchiede zwischen Personen generell. ODer in anderen Worten: die Summe der BIS-Scores innerhalb jeder Person über die Bedingungen ist (bis auf Rundungsfehler) immer gleich und hat den Wert 0. Das kann man einfach überprüfen: dc.m %&gt;% group_by( participant ) %&gt;% summarise(meanBIS=round(mean(bis))) ## # A tibble: 18 × 2 ## participant meanBIS ## &lt;fct&gt; &lt;dbl&gt; ## 1 B006b-08 0 ## 2 B006b-09 0 ## 3 B006b-10 0 ## 4 B006b-11 0 ## 5 B006b-12 0 ## 6 B006b-13 0 ## 7 B006b-14 0 ## 8 B006b-16 0 ## 9 B006b-17 0 ## 10 B006b-18 0 ## 11 B006b-19 0 ## 12 B006b-20 0 ## 13 B006b-21 0 ## 14 B006b-22 0 ## 15 B006b-24 0 ## 16 B006b-25 0 ## 17 B006b-26 0 ## 18 B006b-27 0 9.3 Analyse des BIS-Scores Der BIS-Score ist metrisch und normalerweise parametrisch. Es liegt pro Person und Zelle genau ein Wert vor. Eine gute Wahl für die statistische Analyse ist die rm-Anova, die wir in Teil 8 schon vorgestellt haben. BIS.rmanova &lt;- aov_ez(id = &quot;participant&quot;, dv = &quot;bis&quot;, data = dc.m, within = c(&quot;factor_A&quot;, &quot;factor_B&quot;, &quot;posture&quot;)) BIS.rmanova ## Anova Table (Type 3 tests) ## ## Response: bis ## Effect df MSE F ges p.value ## 1 factor_A 1, 17 0.53 0.84 .004 .372 ## 2 factor_B 1, 17 1.09 56.98 *** .352 &lt;.001 ## 3 posture 1, 17 3.45 78.15 *** .702 &lt;.001 ## 4 factor_A:factor_B 1, 17 0.56 0.12 &lt;.001 .738 ## 5 factor_A:posture 1, 17 0.30 0.01 &lt;.001 .924 ## 6 factor_B:posture 1, 17 0.49 86.16 *** .271 &lt;.001 ## 7 factor_A:factor_B:posture 1, 17 0.31 1.75 .005 .204 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;+&#39; 0.1 &#39; &#39; 1 So ähnlich wie bei der rm-Anova für die RTs sehen wir auch hier Haupteffekte für factor_B, für posture und für die factor_B:posture Interaktion. 9.4 Visualisierung der Ergbenisse des Modells Wieder können wir uns die EMM graphisch anschauen. emm.bis.factorBXposture &lt;- emmeans(BIS.rmanova, ~factor_B:posture) ggplot(as.data.frame(emm.bis.factorBXposture), aes(x=factor_B, y=emmean, color=posture)) + geom_point(position = position_dodge(width=0.5), size=3) + geom_errorbar(aes(ymin=lower.CL, ymax=upper.CL), position = position_dodge(width=0.5), width=0.2) + ylab(label = &quot;BIS score&quot;) Figure 9.1: EMM zur für factor_B und posture 9.5 Post-hoc Vergleich zum BIS-Score Der paarweise Vergleich sieht wie folgt aus: contrast(emm.bis.factorBXposture, method = &quot;pairwise&quot;, adjust = &quot;fdr&quot;) ## contrast estimate SE df t.ratio p.value ## Level_1 uncr - Level_2 uncr 0.226 0.189 17 1.196 0.2479 ## Level_1 uncr - Level_1 crossed 1.649 0.287 17 5.752 &lt;.0001 ## Level_1 uncr - Level_2 crossed 4.052 0.316 17 12.821 &lt;.0001 ## Level_2 uncr - Level_1 crossed 1.423 0.391 17 3.644 0.0024 ## Level_2 uncr - Level_2 crossed 3.825 0.370 17 10.334 &lt;.0001 ## Level_1 crossed - Level_2 crossed 2.402 0.229 17 10.497 &lt;.0001 ## ## Results are averaged over the levels of: factor_A ## P value adjustment: fdr method for 6 tests Wir hoffe, Sie haben gesehen, dass selbst diese etwas komplexeren Auswertemethoden in ihrer Implementierung in R und der Interpretation nicht all zu schwer sind. Im nächsten Teil wollen wir noch einige Möglichkeiten zeigen, wie man “schöne” Abbildungen und Tabellen machen kann. "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
