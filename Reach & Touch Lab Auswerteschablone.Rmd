--- 
title: "Analyse von Verhaltensdaten mit R"
author: "Xaver Fuchs, Christian Seegelke, Tobias Heed"
date: "Letzte Aktualisierung: `r Sys.Date()`"
site: bookdown::bookdown_site
documentclass: book
bibliography: [book.bib, packages.bib]
biblio-style: apalike
link-citations: yes
description: "Eine Handanweisung für die Analyse von behavioralen Daten aus dem Reach & Touch Lab"
---


# Übersicht

## Wozu dieses Dokument dient
Dieses Dokument dient als Schablone für den Ablauf und die Auswertung von BSc/MSc-Arbeiten sowie in Auswertungen im Rahmen eines Empirischen Seminars (ES) in dem Reach & Touch Lab / Kognitive Psychologie. Es enthält Hinweise zur Logik der Auswertung und Experimentaldesign, Wahl der statistischen Verfahren, zeitlichen Ablauf der Asrbeit, und konkrete Code-Beispiele.
Viele Arbeiten werden an einzelnen Stellen von der Schablone abweichen müssen - sie dient nur als Leitfaden. Bitte in jedem Fall mit der Betreuer*in die einzelnen Punkte durchsprechen (ggf. auch Abweichungen vom hier dargestellten Vorgehen).

## Ergänzende Quellen
Es ist ausgesprochen nicht der Anspruch dieses Dokuments, eine umfassende Einführung zu geben. Es soll eher eine Art "Starthilfe" sein. Die hier vorkommenden Themen sind komplex und jedes Kapitel kann für sich dedizierte Bücher füllen--und tut das auch. 
Es gibt einige hervorragende Quellen, zum Vertiefen, von denen hier einige genannt sein sollten. 

Das Thema Datenvisualisierung ist hervorragend in dem Buch von Hadley Wickham [@wickham2009] beschrieben.
Datenanalyse und Visualisierung mit dem sogenannten Tidyverse ist tiefgehend in Hadley Wickhams Buch "R for data science" beschrieben, das auch online aufrufbar ist über die Website von Tidyverse: <https://www.tidyverse.org/learn/>

Zum Thema Datentransformation ("data wrangling") gibt es auch eine sehr anschauliche Serie von Blog-Einträgen auf dieser Website: <https://suzan.rbind.io/2018/01/dplyr-tutorial-1/>

Insgesamt findet man über die üblichen Suchmaschinen im Internet sehr viel gutes Material.

## Hinweis zu diesem Dokument
Dieses Dokument basiert auf RMarkdown. Markdown ist eine einfache Syntax zur Erstellung von HTML, PDF, and MS Word Dokumenten. RMarkown ist eine spezielle Variante von Markdown, bei der Markdown mit R Code kombiniert wird, um reproduzierbare Reports zu erzeugen.

Sie können hier mehr über RMarkdown lesen: <http://rmarkdown.rstudio.com>.

Die wohl ausführlichste Dokumentation zu RMarkdown ist in dem Buch von Yihui Xie [@xie2015] zu finden. Der gleiche Autor hat ebenfalls eine umfassende Bechreibung, die online verfügbar ist: <https://bookdown.org/yihui/rmarkdown/>



## Interaktive Verwendungsweise
Das was dabei für Sie nützlich und wichtig ist, ist dass Sie das Dokument vollständig herunterladen können und auch die dazugehörigen Daten. Das ist sogar exakt was wir Ihnen nahe legen, denn man lernt am besten mit einem "hands-on"-Ansatz. Das heißt, wir wollen Sie ermuntern, das Dokument nicht nur zu lesen, sondern es selbst in RStudio zu öffnen und den Code auf Ihrem eigenen Computer laufen zu lassen. Dabei können Sie den Code nämlich variieren und damit "rumspielen" und sehen, was passiert. So so macht es am meisten Spaß und so lernt man am besten R.

## Rückmeldung zu der Lernerfahrung
Dieses Dokument ist dynamisch und wächst immer weiter. Sie können uns gerne rückmelden, wenn etwas nicht korrekt oder unklar ist, nicht funktioniert oder in einer anderen Weise verbessert werden könnte. 
Für solches Feedback sind wir immer dankbar. Senden Sie dieses Feedback gerne an Xaver Fuchs:
<xaver.fuchs@plus.ac.at>

## Die nächsten Schritte
Zunächst können Sie hier weiterlesen, wie Sie Ihren Computer überhaupt in die Lage versetzen, dass Sie damit mit R und RStudio arbeiten können. 
Danach werden wir Schritt für Schritt verschiedenen Datenverarbeitungsschritte, sowie das Erstellen von Abbildungen und das Durchführen von statistischen Analysen bearbeiten. 



<!--chapter:end:index.Rmd-->

# Einstieg {#get-started}

## R und RStudio
Zunächst brauchen Sie die Programme R und RStudio. Die Programme sind frei verfügbar und für die Plattformen Windows, Mac und Linux verfügbar. 

## Dokument mit ersten Schritten
Wir haben ein pdf-Dokument kompiliert, das Sie durch die Installation und die ersten Schritte mit R führt. 
Wir empfehlen, dass zunächst dieses Dokument bearbeiten und danan zu diesem Dokument zurückkehren. 


Das Einführungsdokument finden Sie [hier](R Intro/Einführung in R.pdf){target="_blank"}.


## Code und Daten herunterladen
Für die interaktive Arbeit mit diesem Dokument benötigen Sie das RMarkdown-Dokument, sowie vier Datensätze. Die Datensätze sind eigentlich identisch, aber in unterschiedlichen Formaten gespeichert. 
Wenn Sie die Browser-Version dieses Buches betrachten, gelangen Sie zu allen Resource (Daten und Code), wenn Sie ganz links oben in der Ecke auf "Auswerteschablone für R" klicken.

### RMarkdown code
Den Code für das gesamte Dokument heißt "Reach & Touch Lab Auswerteschablone.Rmd". Sie finde ihn wie oben beschrieben oder Sie können ihn auch [hier](Reach & Touch Lab Auswerteschablone.Rmd){target="_blank"} herunterladen. 

In dieser Version sind alle Kapitel des Dokuments in einem Dokument. Um den Überblick zu behalten, benutzen Sie am besten die Outline-Funktion in RStudio. Diese finden Sie rechts oben im Skript-Fenster, wo steht "Show document outline"


### Daten
Hier können Sie die Daten herunterladen. Die Daten öffnen sich in einem neuen Tab, aber um Sie herunterzuladen müssten Sie im Kontextmenü (z.B. rechte Maustaste) auf "Ziel speichern unter..." o.Ä. gehen.

* Textfile: [RTL_beispieldaten.txt](Data/RTL_beispieldaten.txt){target="_blank"}
* als Excelfile: [RTL_beispieldaten.xlsx](Data/RTL_beispieldaten.xlsx){target="_blank"}
* als SPSS-File: [RTL_beispieldaten_RTs.sav](Data/RTL_beispieldaten_RTs.sav){target="_blank"}
* als RData-File: [RTL_beispieldaten.RData](Data/RTL_beispieldaten.RData){target="_blank"}

Bitte laden Sie alle vier Datensätze herunter da sonst Teile des Codes nicht funktionieren werden.

## Projektordner anlegen
Legen Sie nun auf Ihrem Rechner einen Projektordner an und kopieren Sie das RMarkdown-Dokument und die vier Datensätze dort hin.




<!--chapter:end:02-get-started.Rmd-->

# Pakete und Daten laden {#read-data}

## Verwendete Packages
Man kann alle Packages zu Anfang laden, oder jeweils dann, wenn man sie im Skript braucht.

### Pakete für Datenverarbeitung und Statistik

Das vorliegende Dokument verwendet *tidyverse*, welches sehr viele sehr nützliche Funktionen für Datenauswertung bereithält.
*afex* und *emmeans* benötigt man nur, wenn man Linear Mixed Models verwendet. Bei BSc/MSc Arbeiten ist dies der Fall, wenn Prozentwert-Daten ausgewertet werden müssen.
*rio* ist ein nützliches Paket, das man braucht, wenn man Datensätze aus anderen Formaten (z.B. Excel oder SPSS) einlesen oder in solchen Formaten speichern will. 
Mit *cowplot* lassen sich leicht mehrere Abbildungen kombinieren.

```{r Packages laden, message=FALSE}
library( tidyverse )
library( afex )
library( emmeans )
library( rio )
library( cowplot )

```

```{r, echo=FALSE}
#manchmal machen Pakete Konflikte, weil zum Beispiel in beiden eine Funktion mit gleichem Namen vorkommt. In diesem Fall kommt "select" sowohl in dem Paket MASS vor, wie auch in dplyr. Wir müssen uns entscheiden, welche Funktion Priorität hat.
select <- dplyr::select
```



### Pakete für RMarkdown
Zusätzlich verwenden wir noch zwei Pakete, die dazu da sind, Tabellen schöner aussehen zu lassen.

```{r Packages laden 2, message=FALSE}
library( knitr )
library( kableExtra )
```


## Daten einlesen

Als nächstes wollen wir die Daten in R einlesen, so dass sie im Workspace sind und man damit  arbeiten kann.

### Verzeichnisse und Pfade
R hat immer einen "Ort", an dem es auf Ihrem Computer ist. Das ist der Pfad (oder das Verzeichnis) von dem aus R "denkt". 

Man kann unterscheiden zwischen  *absoluten Pfaden* und *relativen Pfaden*. Ein Absoluter Pfad fängt mit einer Laufwerksbezeichnung an (auf Windows), zum Beispiel wäre "C:/Programming & Stats stuff/Statistics Tutorial RTL/Daten/Datensatz1.csv" ein absoluter Pfad. Absolute Pfade haben den Nachteil, dass sie dann nicht mehr stimmen, wenn Sie mal etwas in dem Namen (z.B. den Namen eines Ordners) verändern. 

Ein relativer Pfad ist ein Pfad, der relativ formuliert ist zu dem Ort, an dem R gerade "ist" bzw. von wo aus R gerade "denkt". Nehmen wir mal an, der Arbeitspfad (working direectory) von R wäre bei Ihnen gerade auf das Verzeichnis "C:/Programming & Stats stuff/Statistics Tutorial RTL/"
gesetzt, dann wäre der Pfad zu dem Datensatz von oben **relativ** dazu ausgedrückt "Daten/Datensatz1.csv". Der Vorteil von relativen Pfaden ist, dass sie weiterhin stimmen, selbst wenn Sie den Ordner verschieben oder umbenennen, so lange der relative Ort bestehen bleibt (also der Ordern "Daten" mit Inhalt) auch mitverschoben wird. Ein Nachteil ist, dass man den Pfad dann am Anfang der Session mit R auf das gewünschte Verzeichnis setzen muss. 
Ein weiterer großer Vorteil von relativen Pfaden ist, dass Plots, Daten dann auch dort gespeichert (es sei denn man spezifiziert es beim Speichern anders).

In der Summe ist das Arbeiten mit relativen Pfaden sinnvoller und wir werden daher in diesem Dokument immer mit relativen Pfaden arbeiten.


### Verzeichnisse ermitteln und setzen
#### Verzeichnis ermitteln
Sie können einfach herausfinden, wo R gerade "ist", indem Sie folgenden Befehl verwenden:

```{r}
getwd()
```

RStudio hat eine sehr praktische Methode eingebaut, um herauszufinden, an welchem Ort sich das Skript oder Rmarkdown-Dokument befindet, in dem Sie **aktuell** arbeiten. Das kann sehr praktisch sein, da man oft das Arbeitsverzeichnis da hin setzen will, wo das Skript auch ist, um dann zum Beispiel von dort mit relativen Pfaden zu arbeiten. 

Dieser Befehl liest den Pfad des aktuellen Dokuments und schreibt ihn in die Variable "dataPath":
```{r}
dataPath <- dirname(rstudioapi::getActiveDocumentContext()$path)
print(dataPath)
```

#### Verzeichnis setzen
Das Pendant zu "getwd()" heißt "setwd()" und ist dazu da, um ein Verzeichnis zu ändern.

Sie können ganz einfach den Pfad "dataPath", die oben definierte Variable erstellen.
```{r Pfad einstellen, eval=FALSE}
setwd(dataPath)
```

Sie können natürlich den Befehl auch auf einen xbeliebigen anderen (absoluten) Pfad setzen.
Zum Beispiel könnten Sie zunächst definieren:

```{r, eval=FALSE}
dataPath <- "C:/Programming & Stats stuff/Statistics Tutorial RTL/"
setwd(dataPath)
```

Oder auch gleich:
```{r, eval=FALSE}
setwd("C:/Programming & Stats stuff/Statistics Tutorial RTL/")
```

Ersetzen Sie dabei einfach den Pfad auf was auch immer in Ihrer Situation zutrifft.

Hinweis: 
Bei Pfaden gehen die Striche je nach Betriebssystem in die andere Richtung: Mac und Linux verwenden ein /, Windows aber einen \ als Verzeichnistrenner.


## Daten laden

**Zeitplanung: nach den ersten 1-2 Erhebungen**

Die häufigsten Datenformate sind vermutlich:

- Textdateien, in denen jede Spalte eine Variable, und jede Zeile einen Trial enthält
- Exceldateien mit demselben Format
- bereits in R erstellte oder bearbeitete Daten, die im R-Format gespeichert wurden
- SPSS-Dateien, in denen repeated measures nicht über Codiervariablen, sondern über 1 Spalte je Bedingung gespeichert sind

Für alle 4 Fälle wird hier Code zur Verfügung gestellt.

### Daten, die als Textdatei vorliegen

Beim Einlesen von sogenannten comma separated values (csv) Dateien, worunter auch übliche Text-Dateien fallen, ist zu beachten, dass diese Dateien bestimmte Trennzeichen haben können. Üblich sind durch einen Tabulator (Tab) getrennte 
Dateien, durch Komma getrennte Dateien oder durch Punkte getrennte Dateien. Dementsprechend muss man in der Funktion die Flag "sep" anpassen. Im Beispiel ist die Textdatei durch Tabs getrennt, deshalb funktioniert sep = "\t" 

```{r }
#daten laden
bsp <- read.csv("RTL_beispieldaten.txt", sep = "\t")

```

Die Daten sehen in etwa so aus (Ausschnitt)

```{r Daten-glimpse}
bsp %>% select(Participant, Stimuli, Condition, 
               Repetition, SOA, Response.correct, Real_Release_RT) %>% 
  slice(1:10) %>% 
  kable(caption = 'Ausschnitt der Beispieldaten', booktabs = TRUE)
```



### Daten, die in Excel vorliegen


```{r, eval=FALSE}
bsp <- import("RTL_beispieldaten.xlsx", which = 1)
```

Dieser Befehl kommt aus dem Paket *riio* und kann für Exceltabellen auch noch andere Parameter annehmen, z.B. für das Excelblatt (bei Daten mit mehreren Blättern) kann man das zweite Blatt mit "which = 2" einlesen.

### Daten, die in SPSS vorliegen
Das Paket *rio* kann auch verwendet werden, um SPSS-Daten einzulesen.

```{r, eval=FALSE}
bsp_RTs<- import("RTL_beispieldaten_RTs.sav")
```


### Daten, die schon in R gespeichert waren

Hier kann es sein, dass mehr drin ist als man braucht. Also erstmal anschauen.
Dazu den Workspace leeren (in RStudio oben rechts bei Reiter "Environment" den kleinen Besen klicken - das löscht alle vorhandenen Daten).
Anschließend den load-Befehl ausführen. Das, was dann im "Global Environment" gezeigt wird, sind die Daten, die man geladen hat.

```{r Daten laden}
load( "RTL_beispieldaten.RData" )
```

## Daten speichern
Sobald Daten in R importiert wurden, sollten sie im R-Format abgespeichert werden, da dann das Laden am einfachsten ist und alle bearbeiteten Variablen ihre Eigenschaften (z.B. Variablentyp) behalten. Dies gilt insbesondere dann, wenn man die Daten aus einem anderen Format (txt, SPSS) nach R holt.
Wenn Sie an den Daten etwas verändert haben, passen Sie auf, dass Sie nicht ihre ursprünglichen Daten aus Versehen überschreiben. Das können Sie verhindern, indem Sie dem Datensatz einen neuen Namen geben. In unserem Beispiel hängen wir "_export" hinten an den Dateinamen dran. 

### Speichern im R-typischen Format (als RData)

```{r Daten speichern, eval=FALSE}
# Daten für weitere Verwendung in R speichern, zB um Sie seine*r Betreuer*in zu schicken
save( bsp, file = "RTL_beispieldaten_export.RData")
```

### Speichern als Textdatei

```{r, eval=FALSE}
write.table(bsp, file = "RTL_beispieldaten_export.txt", sep = "\t",
            row.names = FALSE, col.names = TRUE)
```

"sep" ist das Trennzeichen zwischen Spalten, t ist ein Tab-zeichen, das ist Standard.

"col.names" exportiert auch die Variablennamen als erste Spalte


### Speichern als als Excel-Datei 
Hierzu kann der "export"-Befehl aus dem Paket *rio* verwendet werden. 

```{r, eval=FALSE}
export(bsp, file = "RTL_beispieldaten_export.xlsx")
```

### Speichern als SPSS-Datei

Auch das SPSS-Format kann mit "export" wie bei den Excel-Dateien gespecihert werden. Bei SPSS ist das weite Format üblich. Da es aber schnell unübersichtlich wird, wird hier beispielhaft nur ein Ausschnitt verwendet, der nur die Reaktionszeiten (RTs) enthält. Die verwendeten Befehle zum Erstellen des Ausschnitts werden unten noch genauer erklärt (und müssen hier nicht im Detail nachvollzogen werden)

```{r}
# Reaktionszeiten über die VP und Reize aggregieren (mitteln)
bsp_RTs <- bsp %>% filter(Repetition==1) %>% dplyr::select(Participant, Stimuli, SOA, Real_Release_RT) %>% group_by(Participant, Stimuli, SOA) %>% summarise(RT=round(mean(as.numeric(Real_Release_RT), na.rm = T), digits = 2)) %>% ungroup()


# den Auschnitt in eine "weites" Format (SPSS-typisch) umwandeln...
bsp_RTs_wide <-  bsp_RTs %>% unite(Stim_SOA, c(Stimuli, SOA)) %>% pivot_wider(names_from = Stim_SOA, values_from = RT)
```

Die Daten im weiten Format sehen dann so aus (Ausschnitt)
```{r Daten-wide-glimpse, tidy=FALSE}
#Datensatz betrachten: Jede VP ist jetzt in einer Zeile (=weites Format, s.u.)
bsp_RTs_wide %>% select(1:10) %>% 
  slice(1:10) %>% 
  kable(caption = 'Ausschnitt der Beispieldaten im weiten Format') %>% 
  scroll_box(width = "100%")
```

Bevor wir als SPSS-Datei speichern, müssen wir die Variablen auch noch ein bisschen umbenenen, weil SPSS es nicht mag, wenn Variablennamen mit einer Zahl beginnen. Da es alles RTs sind, hängen wir einfach mal "RT" vorne an die Namen dran


```{r, eval=FALSE}
VarsOldName <- grep("[[:digit:]]", names(bsp_RTs_wide), value = T)
VarsNewName <- paste("RT", VarsOldName, sep="s_")

bsp_RTs_wide <- bsp_RTs_wide %>% rename_at(vars(VarsOldName), ~ VarsNewName)

export(bsp_RTs_wide, "RTL_beispieldaten_export_RTs.sav") #exportiert als SPSS-Datei
```




<!--chapter:end:03-read-data.Rmd-->

# Analysen planen und Variablentypen anpassen {#plan-analysis}

## Die richtige Analyse für das vorliegende Design wählen
Bevor man mit den Daten losgegt, sollte man sich gut überlegen, wie das Experimentaldesign aussieht und in die Analyse umgesetzt werden muss.
Dazu gehört u.a.:

-   Was ist/ sind meine abhängige Variable/n (AV)?
-   Welches Skalenniveau (nominal, ordinal, kardinal) hat/ haben meine AV? Beachte Abschnitt "Umgang mit %-Daten" weiter unten
-   Was ist sind meine unabhängige Variable/n (UV)? Dies sind i.d.R. unsere experimentellen Faktoren.
-   Welches Skalenniveau hat/ haben meine UV?
-   Welche Ausprägungen/ wieviel Stufen haben meine UV?
-   Handelt es sich um Innersubjektfaktoren (within-subject, abhängige Messungen) und/ oder Zwischensubjektfakoren (between-subject, unabhängige Messungen)?
-   Sind die entsprechenden Variablen in R im korrekten Format (diskrete UV sollten als "Factor" vorliegen, kontinuierliche als "int" (=integer)) oder "numeric". Um die Variablen ins richtige Format zu bringen, s. Abschnitt "Daten ins richtige Format bringen"
-   Sind alle Variablen, die ich benötige schon vorhanden, oder müssen ggf. neue Variablen berechnet werden?

## Daten ins richtige Format bringen

**Zeitplanung: nach den ersten 1-2 Erhebungen**

Nachdem die Daten geladen sind, muss man sie so bearbeiten, dass man sie richtig nutzen kann. Gründe dafür sind, z.B.:

- manche Variablen haben möglicherweise ein ungünstiges Format (z.B. s statt ms)
- R weiß noch nicht, welche Variablen welchen Typ haben; insbesondere muss man R sagen, welche Variablen Faktoren sind, da sonst viele statistische Auswertungen nicht funktionieren.

Was muss man dabei typischerweise beachten?

1. Alles, was man aus einer Textdatei an Texten geladen hat, sind in R zunächst "chr" (steht für character). Dies sind normalerweise in unserer Experimentallogik Faktoren:
    a) Vp-Bezeichnungen => eine Vp ist ein Faktorlevel auf dem Faktor "subject" (bei uns oft subjNr), denn eine Vp wird vielleicht nicht numerisch als 1, 2, ... bezeichnet, sondern komplexer bspw. als B006_s01.
    b) Daten wie "gekreuzt, ungekreuzt" - es ist sinnvoll, wenn man die im Datensatz als Wort belässt; nur muss R wissen, dass man damit Bedingungen meint (und nicht zB einzelne Wörter)
    c) Sonderfall, wenn man unterschiedliche SOA verwendet und diese in die Analyse aufnehmen will: dies sind ja Zahlen (zB 100, 200, 300 ms zwischen einem Prime und einem Stimulus); aber in einer Anova sind es Faktorstufen. Darum muss man sie ebenfalls als Faktor codieren.

2. Häufig enthalten Datenfiles allerlei Spalten, die man gar nicht braucht. Darum erstellt man für Übersichtlichkeit am Schluss ein data.frame, das nur die Sachen enthält, die man wirklich braucht.

Im Folgenden, werden wir step-by-step durch diese Schritte durchgehen.

### Überblick über Datensatz verschaffen
Zunächst muss man sich orientieren: was hat man eigentlich geladen??
Im Folgenden wird angenommen, dass die .RData Daten geladen wurden. Im "Environment" bei RStudio sieht man, dass es ein data.frame "bsp" gibt.

Mit dem Befehl "head" stellt man die ersten Zeilen dar.

```{r}
head( bsp )
```
Man sieht, dass da sehr viel Zeug drin ist.

Mit "str" kann man sich die Struktur des Datensatzes anschauen, d.h., herausfinden was für Variablen er enthält und was für Typen die Variablen haben.

Vorne steht der Variablenname, danach der Typ; z.B. bei Participant "chr", also Text und noch nicht Faktor!
Selbiges gilt für viele andere Variablen auch. Nach der Typangabe sieht man ein par Beispiele, was für Daten genau in der Variable drin sind. 

```{r}
str( bsp ) 
```

Wir sehen, dass bei Participant ziemlich lange Namen drin sind. Da sie informativ sind, wollen wir sie auch gern behalten. Dies ist kein Problem, weil Faktor-Level lange Namen haben dürfen. 
Hingegen ist es für Abbildungen oft praktischer, wenn Faktoren nicht so lange Namen haben, denn die sind dann schlecht lesbar in der Abbildung.
Wir werden unten daher "uncrossed" in der Variable "Condition" umbenennen.

### Kopie des originalen Datensatzes machen
Weil man häufiger zu dem originalen Datensatz zurückkehren möchte (z.B., weil man später doch noch eine Variable braucht, die man vorher schon eliminiert hatte), empfehlt es sich, den originalen Datensatz im Workspace zu behalten und zum Bearbeiten lieber eine Kopie anzulegen. Dazu kopiert man ihn einfach in eine neue Variable. Weil man nicht so viel tippen will, kann man seinen Datensatz am besten sehr kurz benennen. Wir nennen ihn einfach mal "d". 
Das hat den Nebeneffekt, dass man immer vergleichen kann, wie die Daten am Anfang aussahen (bsp) und wie sie aussehen, nachdem man etwas dran gemacht hat (d).

```{r}
d <- bsp
```


### Variablen und Faktorlevels umbenennen
Zu 1a und 1b: Faktoren machen und richtig ordnen
R sortiert von sich aus alphabetisch. Diese Reihenfolge wird dann in Grafiken auch verwendet. Um das so zu haben, wie man es sich wünscht, sortiert man selbst.
Da für das Beispiel der Inhalt des Experiments egal ist, benennen wir die Faktorlevel in Level 1 und Level 2 um.
Wir belassen nur uncrossed/crossed, weil das bei unseren Datensätzen oft vorkommt.

```{r}
d$Participant <- as.factor( d$Participant ) # Faktor machen

d <- d %>% mutate( Condition = recode_factor( as.factor( Condition ), "uncrossed" = "uncr", "crossed" = "crossed" ) ) # vor dem = der alte Name, dahinter der neue

d <- d %>% mutate( StimSide = recode_factor( as.factor( StimSide ), "same" = "Level_1", "different" = "Level_2" ) )

d <- d %>% mutate( StimSegment = recode_factor( as.factor( StimSegment ), "same" = "Level_1", "different" = "Level_2" ) )
```

zu 1c:
Das Bsp-Experiment hatte auch verschiedene SOAs.
Zunächst prüfen wir, welche SOAs vorliegen. 

```{r}
str( d$SOA )
```
Wir schauen mal rein, was für Werte drin sind: Integer-Zahlen 50, 100, 300, 800
Wir erstellen eine zweite Variable, die dieselben Daten aufnimmt.

```{r}
d$soaNum <- d$SOA
```

Anschließend machen wir aus der alten Variable einen Faktor.

```{r}
d$SOA <- as.factor(d$SOA)
```

Warum macht man das? Wenn man SOA in einer ANOVA verwenden will, dann muss man es als Faktor eingeben. Manchmal möchte man Daten aber so plotten, dass die Abstände zwischen den Punkten den echten Wert widerspiegeln. Dazu muss man dann die Integer-Zahlen nutzen. Weil die Integer-Variable jetzt "num" heißt, weiß man, dass man die nur dann nimmt, wenn man keinen Faktor haben will. Wenn man diesen Schritt (Faktor machen) vergisst, erhält man seltsame Fehlermeldungen in ezAnova oder afex.

Man muss sich auf diese Weise alle Daten ansehen und überlegen, welche Spalten man braucht, ob sie im richtigen Format sind etc.

### Abhängige Variablen prüfen und in richtigers Format bringen
Unsere abhängigen Variablen sind, (1) ob eine die korrekte Antwort gegeben wurde udn (2) die Reaktionszeit.

#### AV Antwortverhalten
Die Kodierung für die Antworten liegt noch nicht so vor, wie wir sie gern hätten. 

```{r}
str( d$Response.correct )
```

Da sind Wörter "TRUE, FALSE", und Variablentyp "logi" = binär richtig/falsch.
Wir erstellen daher eine neue Variable und füllen sie zunächst mit 1 für "richtig" oder "TRUE". Überall da, wo gar nicht TRUE drinsteht, sondern FALSE, setzen wir dann eine 0 rein.

```{r}
d$resp <- 1 #zunächst alles auf 1

d$resp[ d$Response.correct != "TRUE" ] <- 0 #Fälle auf 0
```


#### AV Reaktionszeit
Die RT ist auch nicht so, wie wir sie brauchen, denn der "str"-Befehl zeigt an, dass die Variable als "chr" formatiert ist.

```{r}
str(d$Real_Release_RT)
```

```{r}
d$rt <-as.numeric( as.character( d$Real_Release_RT ) )
str(d$rt)
```

Jetzt sind es Zahlen.

## Variablen von Interesse auswählen

Am Schluss erstellen wir einen neuen data.frame, der nur enthält, was wir wirklich brauchen. Damit wird im vorliegenden Fall aus einem data.frame mit 40 Spalten (sehr unübesichtlich) ein viel kürzerer.
Dabei kann man auch gleich noch Namen umbenennen, die einem nicht gefallen.
Z.B: ist die Bezeichnung "Condition" sehr ungünstig, weil sehr unspezifisch. Wir nennen sie hier "Posture".
Für das vorliegende Bsp. nennen wir die anderen Faktoren A und B, da für unser Bsp. egal ist, was da genau getestet wurde.
Wieder machen wir (wie oben) eine Kopie des Datensatzes.
Der neue Name "ds" steht für "data selected" und ist immer noch angenehm kurz.


```{r Variablen auswählen}
ds <- d %>% dplyr::select( participant = Participant, factor_A = StimSegment, factor_B = StimSide, posture = Condition, SOA, soaNum, resp, rt )

```

## Ergebnis überprüfen
Zuletzt sollten wir das  Ergebnis unbedingt anschauen und prüfen, ob alles wie erwartet aussieht

```{r}
head(ds)
```


```{r}
str(ds)
```




<!--chapter:end:04-plan-analysis.Rmd-->

# Daten auf Plausibilität prüfen {#plausibility-check}

**Zeitplanung:**
- **Anzahl Bedingungen, Trials pro Bedingung etc. direkt zu Anfang prüfen**
- **Plotten der Daten einzelner Vpn direkt zu Anfang programmieren**
- **Anzahl Vpn, Gesamtzahl Trials etc. direkt am Ende der Erhebung prüfen!**

Bei jedem Auswertungsschritt sollte man prüfen, ob das, was man in den Daten hat, überhaupt Sinn macht.
Ganz zu Anfang bietet sich an, erstmal zu schauen:

- Denkt R, dass ich so viele Versuchspersonen habe wie ich es auch denke?
- Gibt es pro Vp so viele Zeilen wie ich erwarte (meistens also: so viele Zeilen wie Trials)?
- Gibt es insgesamt so viele Zeilen/Daten wie ich es erwarte?

Außerdem sollte man sich die Daten der einzelnen Vpn mit ggplot plotten (visualisieren) und ansehen. Hierbei sollte man nach ungewöhnlich aussehenden Vpn suchen. 
Ungewöhnlich heißt z.B.:

- eine Vp hat in einer oder allen Bedingungen nahe 100% Fehler. Dies weist meistens darauf hin, dass die Antwortknöpfe vertauscht waren.
- eine Vp hat in einer oder manchen Bedingungen 50% Fehler (oder ein anderes Zufallsniveau, zB 33% bei 3 Wahlmöglichkeiten). Dabei haben alle anderen Vpn nahe 100%. 
- Eine Vp hat ein ganz anderes Antwortmuster (Fehler oder RT) als die anderen
- Eine Vp war viel schneller oder langsamer als alle anderen
- ...

Wie man die die Daten pro Versuchsperson plottet, behandeln wir im nächsten Teil \@ref(basic-plots).

### Anzahl Vpn überprüfen
Der "levels"-Befehl zegt an, welche Levels (also Faktorabstufungen) ein Faktor hat.

```{r}
levels( ds$participant )
```

Wie viele Vpn habe ich im Datensatz? Dazu kann man schauen, wie lang der eben gezeigte Vektor ist.

```{r}
length( levels( ds$participant ) ) 
```
Wir können so sehen, dass 21 Vpn enthalten sind.

### Anzahl von Trials überprüfen

Wenn man weiß, welche Faktoren manipuliert wurden, kann man berechnen, wie viele Zeilen in ds drin sein sollten. 
Hier war das: 

    21 Vpn x 2 (posture) x 2 (Faktor A) x 2 (Faktor B) x 8 SOA x 6 Stimulus-Stärken x 4 Wiederholungen 
    = 21 x 1536 = 32.256 Trials

Nun können wir diese Anzahl vergelcihen mit der Anzahl an Zeilen in unserem Datensatz. 
Theoretisch kann man aauch schauen, was einem im Envirnonment angeziegt wird, denn dort steht ebenfalls die Anzahl an Variablen und Zeilen von data.frames. Die in R eingebaute Funktion hierfür ist aber "nrow".

```{r}
nrow( ds )
```
Wie man sieht: wir haben mehr. Seltsam. Also mal nach einzelnen Vpn aufschlüsseln.

```{r}
( sanCheck <- ds %>% group_by( participant ) %>%
  summarise(
    n()
  ) )
```

Die Funktionen "group_by" und "summarise" sind tidyverse/dplyr-Routinen; sie erstellen nicht einen data.frame, sondern einen "tibble".
Tibbles stellen immer nur einige Zeilen dar. Um alle Zeilen zu sehen, kann man entweder den Tibble als einen dataa.frame anzeigen (auskommentierter Befehl) oder man verwendet die "print" Funktion, mit der man eine beliebge Anzahl an Zeilen darstellen kann und zeigen dabei alle Zeilen an, indem wir die Zeilenanzahl auf "Inf" setzen. 

```{r}
#data.frame( sanCheck )
sanCheck %>% print(Inf)

```
Man sieht, dass Vp15 hat mehr Zeilen hat als erwartet; Vp 28 weniger. Hier muss man also schauen, was los war: Muss man Trials entfernen? Muss man Vpn nacherheben?

Wir schauen uns Vp15 genauer an.
Mit "filter" kann man Zeilen auswählen. Wir wählen alle Zeilen der vp15.

```{r}
ds %>% filter( participant == "B006b-15" ) %>% 
  group_by( factor_A, factor_B, posture ) %>%
  summarize( n() ) %>% 
  print(Inf)
```
Wir vergleichen mal mit einer Vp, die die erwartete Anzahl Zeilen hat:

```{r}
ds %>% filter( participant == "B006b-16" ) %>% 
  group_by( factor_A, factor_B, posture ) %>%
  summarize( n() ) %>% 
  print(Inf)
```
Es ist also im Vergleich zu sehen, dass Vp16 in jeder Faktor-Kombination genau 192 Trials hat; VP15 dagegen hat bei allen "crossed" Bedingungen zu viele Trials. Und das ist noch nicht mal ganz einheitlich. Irgendetwas ist schiefgegangen. Vielleicht wurde der Versuch zwei mal gestartet und es gab einen ersten Druchgang mit crossed, der dann abgebrochen wurde und diese Trials wurden dennoch eingelesen? Um das herauszufinden, müssen die Unterlagen aus der Testung (Testungsprotokoll/Testungstagebuch) geprüft und ggf. der Einlesevorgang wiederholt werden. 
Wenn Sie so eine Situation in den Daten entdecken, wäre das ein typischer Fall, bei dem Sie Ihre* Betreuer*in ansprechen sollten, damit der Daten-Einleseprozess wiederholt wird.
Alternativ gäbe es die Möglcihkeit, die entsprechenden Zeilen zu identifizieren und zu entfernen.
Hier nehmen wir stattdessen an, dass die Vp nicht ausgewertet werden kann, weil wir nicht rekonstruieren können, was da passiert ist.

Man kann mit "filter" diese Vp aus ds entfernen:

```{r}
ds <- ds %>% filter( participant != "B006b-15" ) # beachte, diesmal !=, d.h. wir wählen alles, was NICHT Vp15 ist
```

Als nächstes schauen wir uns Vp28 an:

```{r}
ds %>% filter( participant == "B006b-28" ) %>% 
  group_by( factor_A, factor_B, posture ) %>%
  summarize( n() ) %>% 
  print(Inf)
```
Wir sehen, dass bei Vp28 bei den "crossed" Bedingungen einige Trials fehlen. Da wir auch hier nicht rekonstruieren können, warum das so ist, entfernen wir auch diese Vp aus den Daten (analog zu oben)


```{r Daten auf Plausibilkität prüfen}
ds <- ds %>% filter( participant != "B006b-28" ) 
```

Jetzt, wo einige Daten rausgenommen wurden, sollten die noch vorhandenen Levels im Faktor "participant" gelöscht werden, dazu nimmt man die Funktion "droplevels".


```{r}
ds$participant <- droplevels(ds$participant)
```


**Die Einzeldaten sollen unbedingt mit de_r Betreuer_in durchgesehen und diskutiert werden. Dies sollte unmittelbar nach Ende der Erhebung stattfinden, falls Daten nacherhoben werden müssen!**

<!--chapter:end:05-plausibility-check.Rmd-->

# Daten bereinigen {#clean-data}

Man kann Trials ausschließen, deren RT unter oder über einem bestimmten Wert liegt.
Die zugrunde liegende Idee ist:

- bei sehr schnellen Trials hat die Vp schon im Vorhinein die Antwort vorbereitet, und die Antwort hing daher gar nicht vom Stimulus ab
- bei sehr langsamen Trials hat die Vp nicht richtig aufgepasst oder andere Sachen gemacht, die nicht instruiert waren

Wir schließen häufig Trials aus, die eine RT von <100 oder <150 ms haben, sowie eine RT > 2 * s.d. der jeweiligen Bedingung.
Warum "jeweilige Bedingung"? Schließt man einfach über das ganze Experiment hinweg Trials aus, dann fallen in systematisch langsameren Bedingungen mehr Trials raus als bei schnelleren Bedingungen. Indem man für jede Bedingung separat die 2 s.d. berechnet, wird dieser Bias umgangen.
Ob man überhaupt Trials rausschmeißen soll, ist umstritten. Ein guter Weg ist, die Analyse mit und ohne Rausschmeißen zu machen. Sofern das gleiche rauskommt, braucht man sich keine Gedanken zu machen und es ist egal, welche Version man in die Arbeit nimmt. Mit "gleich" ist dabei gemeint, dass dieselben Bedingungen signifikant werden und alle Schlussfolgerungen bestehen bleiben. Ergeben sich hingegen Unterschiede je nach Rausschmeiß-Strategie, dann sollte man genauer nachforschen, woran das liegt. Es kann bspw. sein, dass es einige sehr wenige Trials mit sehr langer RT gibt, die die s.d. stark erhöhen. Solche Trials sind wahrscheinlich nicht instruktionskonform; dann schmeißt man lieber raus. Findet man keine deutliche Ursache für Unterschiede zwischen verschiedenen Rausschmeiß-Strategien, sollte man mit seine* Betreuer*in sprechen.

## Funktion zur Beseitigung von Trials mit zu langen oder zu kurzen RTs
Um den Bearbeitungsprozess zu erleichtern, werden wir eine selbst-geschriebene Funktion verwenden

Das Prinzip der Funktion ist wie folgt:

- Die Funktion nimmt drei Argumente:
    1) den Vektor an RTs selbt
    2) "upper", ausgedrückt in s.d., als den Toleranzbereich nach oben
    3) "lower", ausgedrückt in ms als die niedrigste RT, die nicht entfernt wird 
- Werte in dem Vektor, die außerhalb dieser definierten Grenzen liegen, werden auf NA gesetzt
- Das heißt, wenn man als upper und lower jeweils 2 und 150 wählt, werden alle Werte, die kleiner sind als 150 ms oder weiter vom Mittelwert entfernt sind als 2 s.d. auf NA gesetzt.

```{r clean funktion}
clean <- function( x, upper, lower ) {
  replace( x, x > mean( x, na.rm=T ) + upper * sd( x, na.rm=T ) | x < lower, NA ) 
}

```

### Test der Funktion an einem Toy-Vektor
Oft ist es sinnvoll, wenn man eine Funktion schreibt, sie erstmal an einem selbst generierten random Beispiel-Vektor zu testen, bevor man sie auf die echten Daten loslässt.

```{r}
toyvector <- c(200, 220, 231, 242, 414, 512, 313, 222, 110, 713, 240, 337)
cat(paste("Der kleinste Wert in dem Vektor ist", min(toyvector)), 
    paste("Der Mittelwert ist", round(mean(toyvector))),
    paste("Die Standardabweichung ist", round(sd(toyvector))),
    paste("Die zweifache Standardabweichung ist", round(2*sd(toyvector))), 
    paste("Der obere cutoff ist", round(mean(toyvector)+2*sd(toyvector))), sep = "\n")

```

Jetzt wenden wir die Funktion auf den Toy-Vektor an:

```{r}
toyvector_clean <- clean(x=toyvector, upper = 2, lower = 150)
data.frame(original=toyvector, cleaned=toyvector_clean)

```
Wir können hier gut sehen, dass zwei Werte, ein neidriger von 110 ms und ein hoher Wert von 713 ms, der mehr als 2 s.d. über dem Mittelwert liegt, auf NA gesetzt wurden. 
Die Funktion scheint also gut zu funktionieren.

Zum Spaß können wir auch die Funktion nochmal mit radikaleren Einstellungen fahren:

```{r}
clean(x=toyvector, upper = 1.2, lower = 220)
```

Wir sehen, dass logischerweise mehr entfernt wird.

## Anwendung der RT-basierten Bereinigung auf die Daten

Jetzt können wir die Funktion auf die Daten anwenden. Wie oben beschrieben, wird die Funktion auf jede Bedingung einzeln angewendet. Als Regel nehmen wir aber immer 

    enfernen, wenn RT > 2 s.d. über Mittelwert ODER < 150 ms.

Dass der Befehl einzeln für jede Bedingung ausgeführt wird, liegt an dem "group_by"-Befehl, der R sagt, dass es eine Funktion separat für jedes Subset ausführen soll. Als ein Subset bezeichnet man die Daten, die zu einer Kombination der Faktoren in dem "group_by"-Befehl gehören.
Den neuen Datensatz nennen wir "dsClean".

```{r}
dsClean <- ds %>% group_by( participant, factor_A, factor_B, posture, SOA ) %>%
  mutate(rtClean = clean( rt, 2, 150 ))
```

## Korrektur der AV Antwort für die Trials mit ungültigen RTs

Für die Trials mitt einer zu schnellen oder zu langsamen RT, die wir in dem vorherigen Schritt identifiziert haben, sollten wir auch die Antwort der Vpn entfernen, da diese ebenfalls ungültig sind. 
Wir legen hierzu eine neue Variable "respClean" an.

```{r}
dsClean$respClean <- dsClean$resp #zunächst gleich wie rep
dsClean[ is.na( dsClean$rtClean ), "respClean"] <- NA #Fälle auf NA
```

## Anzahl entfernter Trials berichten

Jede empirische Arbeit sollte klar kommunizieren, wie bei der Bereinigung von Daten vorgegangen worden ist und wie viele Trials dabei entfernt wurden.

Die Anzahl der in dem Schritt oben entfernten Daten lässt sich einfach bestimmen.
Das ist deshalb möglich, weil die Daten vor der Korrektur noch vorhanden sind. 

```{r}
print( paste( "Anzahl Trials gesamt:", nrow( ds ) ) )
print( paste( "Anzahl Trials nach Cleaning: ", nrow(dsClean[ !is.na( dsClean$rtClean ), ] ) ) )

print( paste( "Prozent eliminiert:", round( 100 - nrow(dsClean[ !is.na( dsClean$rtClean ), ] ) / nrow( ds ) * 100, digits = 1 ) ) )

```

Dieser Wert liegt im Bereich des Erwartbaren. Dadurch, dass oberhalb von 2 s.d. die Daten abgeschnitten wurden, gehen automatsch 4.5 % der Daten verloren. Dass die Prozentzahl daann noch etwas höher ist, liegt daran, dass auch wegen des unteren cutoff-Kriteriums bei 150 ms noch einigen Trials entfernt werden. 

## Datensatz auf gültige Trials beschränken

Nachdem wir berechnet und berichtet haben, wie viele Trials ausgeschlossen wurden, kann man einen neuen Datensatz machen, der die ungültigen Trials nicht mehr enthält. Das vereinfacht viele Operationen, da R sonst häufig Berechnungen wegen fehlnder Werte nicht durchführt oder in Funktionen explizit gemacht werden muss, wie mit NA umgegangen werden soll.
Den neuen Datensatz, der nur noch Daten enthält mit denen wir rechnen wollen, nennen wir einfach "dc" (kurzer Name).

```{r}
dc <- dsClean %>% filter(!is.na(rtClean))

```

Nun überprüfen wir noch einmal, ob alles stimmt. Wir wollen nicht, dass noch irgendwo NAs sind.

```{r}
sanityCheck <- dc %>% group_by( participant, factor_A, factor_B, posture, SOA ) %>%
  summarize( N = sum( is.na( resp ) ))

sum( sanityCheck$N > 0 )
```

Es ist also alles in Ordnung. Wir können nun mit der Statistik beginnen. 

<!--chapter:end:06-clean-data.Rmd-->

# Visualisierung der Rohdaten {#basic-plots}

**Zeitplanung: Abbildungen der Rohdaten sind ein elementarer Teil von Plausibilitäts-Checks. Man sollte sich sofort damit befassen, wenn man die Daten hat**


## Grammar of Graphics
Das Prinzip der "Grammar of Graphics" ist ausführlich in Hadley Wickhams [@wickham2009] beschrieben und sprengt den Rahmen dieses Dokuments. Das Wichtigste ist aber:

- Die Eigenschaften des Plots nennt man "Aesthetics"
- Beispiele für Aesthetics sind:
    - x- und y-Richtung
    - Farbe von Elementen
    - Form
    - Größe
    - Linienart
- Beispiele für Eigenschaften der Daten sind typischerweise Ausprägungen von (diskreten oder kontinuierlichen) Variablen, z.B.:
    - Eine kontinueriliche AV Variable wie Reaktionszeit
    - Eine diskrete AV wie Antwort korrekt (ja/nein)
    - Eine diskrete UV, wie ein Bedingungs-Faktor
- In Datenvisualisierungen (Plots) werden dann Eigenschaften aus der Daten auf bestimmte Eigenschaften des Plots "gemappt" (aesthetic mapping)
- Grammar of Graphics bedeutet, dass man *eine* Eigenschaft der Daten auf *eine* Aesthetic des Plots mappt. Man würde also zum Beispiel die RTs auf die y-Achhse des Plots "mappen" und die Bedingung (sagen wir "crossed" vs. "uncrossed") als Farbe ausdrücken aber man würde nicht RTs sowohl in y-Position wie auch in dem Farbton ausdrücken. Mapping sollte immer Eins-zu-Eins sein. Es mag natüerlich sein, dass es Situationen gibt, wo man etwas ganz bestimmtes mit einem Doppel-Mapping bezweckt. Es ist nicht "verboten", das zu tun, aber man sollte es nicht unnötig tun. 
- Man sollte nach Möglichkeit mit dem Mapping konsistent (das heißt, auch über Plots hinweg) sein. D.h., man sollte nicht in Abbildung 1 die Bedingung als Farbe und in Abbildung 2 die Bedingung plötzlich als Form ausdrücken.

## Visualisierung von Rohdaten pro Vpn

Das erste, was man tun sollte, ist, dass man sich die Daten nocheinmal gut anschaut und überprüft, ob die Daten plausibel aussehen. Was das bedeutet, haben wir im letzten Teil beschrieben \@ref(plausibility-check).

### Visualisierung der AV Antworten

Welche Art von Plot man macht, hängt von vielen Parametern des Designs und auch der Stichprobe ab.
Da wir ein within-subject Design mit einer relativ kleinen Stichprobe (19 nach Enfernung von 2 Vpn) haben, bietet es sich sich, einzelne Plots für die Personen zu machen. Dies geht mit der "facets"-Funktion von ggplot2.

Zunächst berechnen wir aber mal pro Vpn denn Prozentsatz richtiger Antworten.
Es bietet sich an, für komplexere Plots einen eigenen Datensatz zu machen. 
Wir nennen diesen mal "dp" (für data plot).

```{r raw-per-Vp-response, fig.cap="Anteil korrekter Antworten für jede Vp"}
dp <- dc %>% 
  group_by(participant, factor_A, factor_B, posture, SOA) %>% 
  summarise(percCorr=mean(respClean)*100)

ggplot(dp, aes(x=SOA, y=percCorr, shape=factor_A, linetype=factor_B, color=posture)) +
  geom_point() +
  geom_path(aes(group=interaction(participant, factor_A, factor_B, posture))) +
  facet_wrap(~participant)
```

Das erklärte "Mapping" von Daten auf "Aesthetics" ist (vor allem) in der ersten Zeile des Plot-Befehls in der Funktion "aes" zu sehen. Etwas komplexer ist der Befehl innerhalb von "geom_path" (was Linien macht). Dieser Parameter (group) definiert, welche Beobachtungen zusammen eine Gruppe ergeben. Mit "interaction" sagt man hier, dass die Gruppe die Kombination betrifft aus den genannten Faktoren. Das sind in dem Fall alle außer SOA, denn die Gruppe wird ja über SOA geplottet.

Der Plot ist überaus informativ. Es sind sofort Dinge zu sehen, von denen wir bisher gar nichts wussten. Das Wichtigste:

- VP 16 und 27 sind in den crossed Bedingungen in ihrer Performance verdächtig nahe an 50% (der Ratewahrscheinlichkeit). Da sie aber in den anderen Bedingungen gut performen und auch bei längeren SOA besser sind, kann man aber davon ausgehen, dass sie schon instruktionskonform agiert haben, aber einfach diese Bedingungen sehr schwer fanden (einen besonders starken crossing effect zeigten). Wir lasse die Personen also drin. 
- VP 23 hat Leistungen von fast 0 Prozent. Das ist praktisch unmöglich bei einer Ratewahrscheinlichkeit von 50%. Es würde ja bedeuten, dass die Person exakt entgegengesetzt den Instruktionen gedrückt hat und darin sogar sehr gut war. Es ist sehr wahrscheinlich, dass die Knöpfe falschrum angeschlossen oder hingelegt wurden oder die Instruktion umgekehrt war als sie sollte. Da wir das aber nur aus dem Protokoll entnehmen können, bleibt uns zunächst nichts anderes übrig, als die Person zu entfernen.

```{r}
dc <- dc %>% filter(participant != "B006b-23")
dc$participant <- droplevels(dc$participant)
```


### Visualisierung der AV RTs
Als nächstes machen wir einen ähnlichen Plot für die Reaktionszeiten. 

```{r raw-per-Vp-RT, fig.cap="Reaktionszeiten für jede Vp"}
dp <- dc %>% 
  group_by(participant, factor_A, factor_B, posture, SOA) %>% 
  summarise(meanRT=mean(rtClean))

ggplot(dp, aes(x=SOA, y=meanRT, shape=factor_A, linetype=factor_B, color=posture)) +
  geom_point() +
  geom_path(aes(group=interaction(participant, factor_A, factor_B, posture))) +
  facet_wrap(~participant)
```

## Visualisierung auf Gruppenebene
Als nächstes wollen wir die Daten für die Stichprobe plotten.

Wir gehen ähnlich vor wie oben, wir lassen aber "participant" sowohl bei dem "group_by" wie auch in den "facet_wrap" weg.

### AV Antwort

```{r raw-group-response, fig.cap="Anteil korrekter Antworten in der Gruppe"}
dp <- dc %>% 
  group_by(factor_A, factor_B, posture, SOA) %>% 
  summarise(percCorrect=mean(respClean)*100)

p1 <- ggplot(dp, aes(x=SOA, y=percCorrect, shape=factor_A, linetype=factor_B, color=posture)) +
  geom_point() +
  geom_path(aes(group=interaction(factor_A, factor_B, posture)))
p1
```

In dieser Abbildung können wir nun schon einiges über die Ergebnisse lernen, z.B.:

- dass "crossed" zu mehr Fehlern führt
- dass bei Faktor_B vor allem "Level_2" zu mehr Fehlern führt
- dass Faktor_A weniger auszumachen scheint

Das sind natürlich erstmal nur deskriptive Beobachtungen. Ob diese Effekte statistisch signifikant sind, wird man erst nach der Statistk sagen können.

### AV RT
Zuletzt schauen wir uns noch die RT Daten an.

```{r raw-group-rt, fig.cap="Miittlere RTs in der Gruppe"}
dp <- dc %>% 
  group_by(factor_A, factor_B, posture, SOA) %>% 
  summarise(meanRT=mean(rtClean))

p2 <- ggplot(dp, aes(x=SOA, y=meanRT, shape=factor_A, linetype=factor_B, color=posture)) +
  geom_point() +
  geom_path(aes(group=interaction(factor_A, factor_B, posture)))
p2
```

Hier kann man deskriptiv sehen, dass in den RT ein inverses Pattern vorliegt wie bei den Antworten. Bei crossed und bei Level_2 von factor_B sind die Antworten langsamer. 


## Daten abspeichern
Da jetzt die Plausibilitätsprüfung abgeschlossen ist, werden wir die bereinigten Daten erneut abspeichern, so dass sie später leichter wie geladen werden können. 

```{r}
save( ds, dc, file = "RTL_beispieldaten_clean.RData")
```







<!--chapter:end:07-basic-plots.Rmd-->

# Statistische Analysen {#basic-stats}

## Vorwort zu statistische Analysen 
Bevor wir dezidiert an die Analysen der Antwort- und RT-Daten gehen, hier ein paar allgemeine Informationen vorweg.

### Statistische Auswertung von %-Daten (zB korrekt/inkorrekt)
Prozentwerte darf man nicht mit einer Anova analysieren. Details dazu finden sich in:
Jaeger, T. F. (2008). Categorical data analysis: Away from ANOVAs (transformation or not) and towards logit mixed models. Journal of Memory and Language, 59(4), 434–446. https://doi.org/10.1016/j.jml.2007.11.007

Stattdessen analysiert man sie mit generalized linear mixed models (GLMM). Diese modellieren die Wahrscheinlichkeit, dass ein Trial korrekt oder inkorrekt beantwortet war. Dies passiert auf Ebene eines einzelnen Trials: Statt Prozentwerte zu berechnen, wie viele Trials richtig waren, beantworten GLMM die Frage, wie wahrscheinlich ein gegebener Trial korrekt oder inkorrekt beantwortet wird, gegeben die Faktorausprägungen des jeweiligen Trials. Dabei ändert also ein Faktor die Wahrscheinlichkeit, dass man richtig oder falsch antwortet; dadurch ändert sich natürlich indirekt auch die Prozentzahl richtiger/falscher Trials.

Im Rahmen von BSc/MSc-Arbeiten sind v.a. folgende Aspekte wichtig:

1) Prozentwerte nicht mit Anova auswerten, sondern korrekt mit GLMM; wir nutzen dazu das package afex.
2) Dies zieht andere posthoc-Tests nach sich; wir nutzen dazu das package emmeans.
3) Ein wesentlicher Punkt bei GLMM für korr/inkorr Daten ist, dass Veränderungen im Bereich nahe 50% (also Zufallsniveau) sehr viel wahrscheinlicher sind als Veränderungen im Beriech 100% (also perfekte Performance). Das heißt, Veränderungen bei GLMM sind nicht linear im Prozentbereich. Stattdessen sind sie linear auf der Skala, die das GLMM benutzt - dies ist eine logit Skala (also, wer sich damit nicht genauer auseinandersetzen will, der muss das auch nicht - aber es hat Konsequenzen füt die Darstellung, s.u.)
Die Nichtlinearität auf der Prozent-Skala wirkt sich darin aus, dass error bars nicht mehr symmetrisch sind. Wie kann das sein? Das GLMM schätzt lineare Effekte auf der logit-Skala. Dort gibt es dann auch ganz normale, lineare s.d. und s.e. Aber wenn man diese dann in Prozentwerte transformiert, ist diese Transformation nicht linear. Wie man das macht, findet sich hier im Skript.


### Statistische Auswertung von metrischen Daten mit Anovas und linear mixed models
Kontinuierliche oder metrische Daten werden mit einer Anova oder mit einem linear mixed model (LMM) ausgewertet. Dabei ist zu beachten, durch was für eine Art von Design die Daten zustande gekommen sind. Wenn es sich um eine abhängiges, within-subject Design handelt, darf keine "normale"" Anova gerechnet werden, sondern es muss eine repeated-measures Anova (rmAnova) gerechnet werden, die berücksichtigt, dass mehre Daten (z.B. aus unterschiedlichen Bedingungen) von der gleichen Person kommen. 
In einem solchen Fall besteht auch die Möglichkeit, anstatt einer rmAnova ein LMM zu rechnen. Der Unterschied zwischen einer rmAnova und einem LMM bestehen in den sogenannten random effects, wie random intercepts und random slopes. LMMs "erlauben" Versuchspersonen, ein unterschiedliches Ausgangsniveau (random intercept) und auch unterschiedlich starke Steigungen von Regressionsgeraden zu haben (random slope). An der Terminologie kann man schon erkennen, dass LMMs eher eine Erweiterung von multiplen und gemischten (mehrere Faktoren und kontinuierliche Prädiktoren) Regressionen sind. Insofern ist hier auch der Begriff "Regressionsgerade" etwas weiter zu fassen und eher zu begreifen als ein Koeffizient, der auch die Veränderung zwischen (zwei oder mehr) Faktorabstufungen angibt. LMMs haben gegenüber der rmAnova einige Vorzüge, dazu zählen

- ein besserer Umgang mit fehlenden Datenpunkten 
- eine hohe Felxibilität in der Modellformulierung, bei der von sehr einfachen bis komplexen Modellen alles innerhalb des gleichen Frameworks berechnet werden kann

Ein wichtiger Unterschied und auch Vorzug ist auch, dass ein LMM durch die Verwandtschaft zu Regression ein echtes Modell ist, das heißt, dass es Daten fittet und damit Vorhersagen macht. Unterschiedliche Modelle können ebenfalls gegeneinander getestet werden, um herauszufinden, welches Modell angemessener ist.

### RTs als metrische Daten
In dem hier behandelten Beispieldatensatz sind Reaktionszeiten (RTs) enthalten. Wir werden sie als metrische Daten fassen und mit einer Anova auswerten. Ob RTs tatsächlich metrische Daten sind oder nicht, ist eine fortlaufende Diskussion und es wurde immer wieder der Einwand gebracht, dass sie transformiert (zum Beispiel log-transformiert) werden sollten, da sie häufig eine schiefe Verteilung haben. Allerdings lässt sich auch zeigen, dass die Transformationen Gefahren bergen und, dass von ihnen eher abgesehen werden sollte. Mehr Informationen findet man in folgendem Artikel:

Schramm, P., & Rouder, J. (2019). Are Reaction Time Transformations Really Beneficial? [Preprint]. https://doi.org/10.31234/osf.io/9ksa6

Wir befürworten daher eher eine Auswertung von Reaktionszeiten, wie sie sind, anstatt einer Transformation.

### RTs bei korrekten und inkorrekten Trials
Es ist in der Literatur nicht ganz einheitlich (und kommt wahrscheinlich auf das Design an), ob Reaktionszeiten auf der Grundlage von allen Trials und ungeachtet der Frage analysiert werden, ob die Antwort in dem Trials korrekt war oder nicht. Es gibt aber gute Gründe zu argumentieren, dass ein Fehler etwas anderes ist als eine korrekte Antwort und, dass die Kombination von RTs von korrekten Antworten und Fehlern zwei Verteilungen vermischen würde. Demnach wäre es aussagekräftiger Fehler und korrekte Antworten separat zu analysieren oder sich die RTs halt nur für die korrekten Antworten anzuschauen. 
Das ist der Ansatz den wir hier verfolgen. 
Bei der Erstellung des Datensatzes für die Analysen der RTs werden also immer die inkorrekten Antworten ausgeschlossen.

## Die "normale" Anova
Bei dem bestehendem Datensatz bietet sich eine Anova als Auswertung der Verhaltensdaten nicht an, da es sich um ein within-subject Design handelt. 
Die "normale" Anova wird verwendet, um beispielsweise zu testen, ob sich drei distinkte Gruppen bezüglich einer metrischen Variable unterscheiden. Dass eine VP mehrmals vorkommt, ist nicht erlaubt, da es die Annahme der Unabhängigkeit der Beobachtung verletzt. 
In diesem Beispiel wird daher analysiert, ob die im Experiment verwendete Körperstellung (welcher Arm oben lag) und das Geschlecht (weiblich/männlich) einen Effekt auf die mittlere Reaktionszeit (gemittelt über alle Bedingungen und SOAs) hat.


### Daten vorbereiten
Zunächst generieren wir einen passenden Datensatz. Hierzu muss die Information aus dem Datensatz "bsp_demo" mitverwendent und an den andern Datensatz angehängt werden.

```{r Anova}
# mittlere Reaktionszeiten der korrekten Antworten berechnen 
dAnova <- dc %>% filter(respClean==1) %>% group_by(participant) %>% summarise(n=n(), meanRT=mean(rt))

# Anfangsbuchstaben der Variablennamen klein machen
bsp_demo <- bsp_demo %>% rename_all(tolower)

# Info aus den demographischen Daten hinzufügen
dAnova <- right_join(bsp_demo, dAnova)

# Variablen korrekt als Faktoren definieren
dAnova$sex <- factor(dAnova$sex)
dAnova$arm_top <- factor(dAnova$arm_top)
```

### Der Anova-Befehl und Output
Als nächstes rechnen wir die Anova mit dem Befehl "aov_ez" aus dem *afex* Paket. 
und bertrachten den Output.
```{r}
meanRTs.anova <- aov_ez(id = "participant", dv = "meanRT", data = dAnova, between = c("sex", "arm_top"))

```
Nun betrachten wir den Output der Anova, den sogenannten Anova-Table:

```{r}
meanRTs.anova
```

Wie man der Anova-Tabelle entnehmen kann, hat keiner der beiden Faktoren Geschlecht und Armstellung einen signifikanten Effekt auf die Reaktionszeit. Auch die Interaktion ist nicht signifikant.
Im Normalfall ist die Analyse dann an dieser Stelle beendet, da in der Situation eines nicht signifikanten Tests in der Anova, keine direkten Vergleiche mehr angestellt werden. Für den Fall, dass aber einer der Faktoren oder auch die Interaktion signifikant gewesen wäre, hätte man einen post-hoc Test gerechnet, um herauszufinden, welche der Gruppen sich unterscheiden. 

### Post-hoc Vergleiche
Wir werden aus Demonstrationsgründen hier einfach den post-hoc Test trotzdem rechnen. Dazu verwenden wir den Befehl "emmeans" aus dem gleichnamigen Paket.

Das Paket emmeans ist sehr mächtig, es ist aber auch nicht ganz einfach in der Handhabung. 
Es lohnt sich für einen vertieften Einstig entweder die Vignette des Pakets unter <https://cran.r-project.org/web/packages/emmeans/vignettes/comparisons.html> oder diesen Blog <https://aosmith.rbind.io/2019/03/25/getting-started-with-emmeans/> zu lesen. 


Zunächst schauen wir uns den Haupteffekt von "sex" an:
```{r}
emmeans(meanRTs.anova, pairwise ~ sex, adjust = "fdr")
```

Nun schauen wir uns die Interaktion von sex und arm_top an.
Wenn, wie häufig bei Anova mehr als zwei levels bei einem Faktor vorkommen, dann sollte man unbedingt p-Werte von post-hoc Tests korrigieren. Als Methoden bieten sich Bonferroni oder False Discovery Rate (fdr) an. Diese Methoden lassen sich bei emmeans einfach anwenden indem sie bei dem "adjust"-Parameter definiert werden.

```{r}
emmeans(meanRTs.anova, pairwise ~ sex:arm_top, adjust = "fdr")

#alternativer code:
#emm.sex_armtop <- emmeans(meanRTs.anova, ~ sex:arm_top)
#contrast(emm.sex_armtop, method = "pairwise", adjust = "fdr")
```

## Die repeated measures Anova
Im Gegensatz zu der "normalen" Anova, ist eine repeated measures anova (rm-Anova) dazu geeignet, um Daten auszuwerten, die mittels eines within-subject-Designs (das heißt mit mehren Messpunkten pro Person; wie bei dem gegebenen Datensatz) gewonnen wurden. 

Im Folgenden wird ein Datensatz erstellt, bei dem zunächst die RTs über die SOAs innerhalb jeder Person gemittelt werden. Dann wird der Einfluss von factor_A (gleich/ungleich) und factor_B (gleich/ungleich) und posture (crossed/uncrossed) auf die RTs analysiert.

### Daten vorbereiten
Da auch rm-Anova pro Person und Faktorkombination nur einen Wert erwartet, berechnen wir die mittlere Reaktionszeiten der entsprechenden Faktorkombinationen für die Vpn.

```{r rmAnova}
drmAnova <- dc %>% filter(respClean==1) %>% 
  group_by(participant, factor_A, factor_B, posture) %>% 
  summarise(meanRT=mean(rtClean))
```
Bei rm-Anova sollte man auch überprüfen, dass es auch keine missing values gibt, weil das problematisch ist. 
Das heßt, es sollte für jede Faktorkombi pro Person genau ein Wert da sein. 
Das kann man mit dem Befehl "table" prüfen.

```{r}
with(drmAnova, table(participant, factor_A, factor_B, posture)) 
```

Das sieht alles richtig aus. 

### Anova-Befehl und Output
Der wichtige Unterschied zu vorher ist der "within" Faktor.

```{r}
meanRTs.rmanova <- aov_ez(id = "participant", dv = "meanRT", data = drmAnova, within = c("factor_A", "factor_B", "posture"))

#Ergebnisse in der Konsole anzeigen lassen
meanRTs.rmanova
```

Wie man in der Anova Tabelle sieht, sind "factor_B", "posture" und auch die Interaktion aus "faktor_B" und "Posture" signifikant. Die Lage ist also schon ein Stück komplexer geartet. 


### Plots von estimated marginal means zum Nachvollziehen der Interaktion
Im Folgenden wird ein graphischer Eindruck über die Werte vermittelt und die Interaktion heruntergebrochen. Das Paket *emmeans* kann uns hierbei wieder weiterhelfen. Es ist nämlich nicht nur dazu da, um post-hoc Tests zu rechnen, sondern auch, um estimated marginal means (EMM) zu berechnen. EMM sind vom Modell geschätzte Werte übder die Randhäufigkeiten (also für Faktoren und Faktorkombinationen).

Beim Herunterbrechen einer Interaktion mittels EMM fängt man normalerweise mit der höchsten signifikanten Kombination an. In diesem Fall ist das die aus factor_B und posture.

```{r rmAnova_marginalMeans}
emm.factorBXposture <- emmeans(meanRTs.rmanova, ~factor_B:posture)
print(emm.factorBXposture)
```

Die EMM kann man auch plotten, um sich einen besseren Überblick zu veschaffen:
```{r}
ggplot(as.data.frame(emm.factorBXposture), 
       aes(x=factor_B, y=emmean, color=posture)) + 
  geom_point(position = position_dodge(width=0.5), size=3) + 
  geom_errorbar(aes(ymin=lower.CL, ymax=upper.CL), position = position_dodge(width=0.5), width=0.2) + 
  ylab(label = "reaction time [ms]")
```


In der Abbildung kann man sehen, dass die Reaktionszeiten insgesamt bei der "crossed" posture höher sind, was den in dem Anova-Table sichtbaren Haupteffekt von "posture" reflektiert. Zudem kann man sehen, dass die Reaktionszeit bei "Level_2" für den factor_B bei "crossed" erhöht ist und bei uncrossed nicht (hier ist die RT sogar etwas niedriger). Dieses Verhältnis spiegelt die signifikante Interaktion wieder. 

### Post-hoc paarweiser Vergleich für die factor_B x posture Interaktion
Die Interaktion wird nun noch mit post-hoc Tests runtergebrochen, um zu prüfen, welche der Unterschiede signifikant sind. 

```{r}
contrast(emm.factorBXposture, method = "pairwise", adjust = "fdr")

```

Wie man in der Tabelle sehen kann, sind also alle Vergleiche zwischen den Stufen signifikant mit Ausnahme des Vergleiches zwischen "Level_1"" und "Level_2"" bei "uncrossed". Somit lassen sich sowohl der Haupteffekt wie auch die Interaktion mittels der post-hoc Vergleiche nachvollziehen. 


## Das linear mixed model (LMM)
Zuletzt wird noch das LMM vorgestellt, das sich auch gut eignet, um metrische repeated measures Daten zu analysieren. 
Trotz der mathematischen Unterschiede zu einer rm-Anova, lässt sich der Output recht ähnlich interpretieren. 

Ein Unterschied ist, dass LMMs in der Regel auf allen Daten beruhen und man nicht vorher, wie oben, über bestimmte Levels aggregiert (Mittelwerte gebildet) werden müssen. Hier werden wir also einfach alle SOAs und alle Wiederholungen der Trials des gleichen Typs "reinfüttern".
In der Anwendung ist ein Unterschied, dass das LMM mit dem afex-Befehl "mixed" angesprochen wird und, dass man die Faktorstruktur über eine Formel angibt.

Sie können in dem Befehl unten (im nächsten Abschnitt) sehen, dass einige Parameter gesetzt werden, die Sie noch nicht kennen. 

Das eine ist der sogenannte *random effect*. Hier haben wir ihn definiert als "(1|participant)". Diese Art von random effect nennt man einen random intercept für den Faktor Versuchsperson. Kurz ausgedrückt bedeutet das, dass das Modell berücksichtigt, dass die Versuchspersonen einen eigenen Ausgangspunkt haben. Das ist die minimale Annahme, die wir für ein LMM machen können. 
Die korrekte Formulierung der random effects ist eine der größeren Kontroversen zu dieser Art von Modellen. 

Diese Kontroverse ist z.B. in folgendem Artikel behandelt:
Barr, D. J., Levy, R., Scheepers, C., & Tily, H. J. (2013). Random effects structure for confirmatory hypothesis testing: Keep it maximal. Journal of Memory and Language, 68(3), 255–278. https://doi.org/10.1016/j.jml.2012.11.001

Der andere Parameter, der für Sie neu sein dürfte ist "method". Hier wird eine von mehreren zur Verfügugn stehenden Methoden für die Schätzung der Freiheitsgrade definiert. 
Es gibt:

- Kenward-Rogers "KR": langsam
- Satterthwaite "S": etwas schneller
- Asymptotic: das schnellste, schätzt df auf Inf

Wir nehmen hier "S".

### Daten vorbereiten
Wie oben beschreiben, wird nicht aggregiert, wir sortieren aber wieder die falschen Antworten raus.

```{r}
dLMM <- dc %>% filter(respClean==1)
```


### Mixed-Befehl und Output

```{r LMM}
meanRTs.lmm <- mixed(rtClean~factor_A*factor_B*posture + (1|participant), data = dLMM, method = "S") 
```

Den Output schaut man wie zuvor an:
```{r}
meanRTs.lmm
```


Auch beim LMM bekommt man einen Anova-Table. Der Output lässt sich genau so interpretieren, wie bei der Anova. Auch hier bekommen wir gezeigt, dass es einen signifikanten Haupteffekt von "factor_B" und "posture" gibt. Und eine Interaktion aus "factor_B" und "posture", wie bei der rmAnova. Einziger Unterschied ist hier, dass das LMM auch noch eine (gerade so) signifikante Interaktion zwischen "factor_A" und "factor_B" anzeigt. Der folgende Plot soll aufzeigen, was es mit den Interaktionen auf sich hat. 

### Plots von EMM für die factor_B x posture Interaktion
Wir beginnen zunächst wieder mit der Interaktiion aus "factor_B" und "posture", die wir schon aus der rm-Anova kennen.

Bevor wir die emms berechnen, sagen wir R noch, welche Methoden verwendet werden sollen, um die Freiheitsgrade zu schätzen. Das Paket *emmeans* kann mehrere verwenden, die sich mathematisch unterscheiden und teilweise sehr unterschiedlich lange Berechnungsdauern haben (ähnlich wie oben). Wir wählen die Methode, die "asymptotic" heißt.

```{r}
#options für df schätzung bei post hoc tests
emm_options(lmer.df = "asymptotic") # options: 'satterthwaite', 'kenward-roger', 'asymptotic'

#Wenn Sie 'satterthwaite' verwenden wollen, müssen Sie das Limit an Tests hochsetzen
#emm_options(lmerTest.limit = 30000)
```


```{r LMMMarginalMeans}
emm.lmm.factorBXPosture <- emmeans(meanRTs.lmm, ~factor_B:posture)

ggplot(as.data.frame(emm.lmm.factorBXPosture), 
       aes(x=factor_B, y=emmean, color=posture)) + 
  geom_point(position = position_dodge(width=0.5), size=3) + 
  geom_errorbar(aes(ymin=asymp.LCL, ymax=asymp.UCL), 
                position = position_dodge(width=0.5), width=0.2) + 
  ylab(label = "reaction time [ms]")

```

Das sieht im Wesentlichen aus wie oben bei der rmAnova.

### Post-hoc paarweiser Vergleich für die factor_B x posture Interaktion
Das Vorgehen ist identisch wie bei der rm-Anova:
```{r LMMPostHoc}
contrast(emm.lmm.factorBXPosture, method = "pairwise", adjust = "fdr")

```

Auch die Vergleiche fallen von der Interpretation her ähnlich aus wie oben bei der rmAnova. Es fällt aber ins Augem, dass auch der erste Vergleich zwischen Level_1 und Level_2 bei uncrossed signifikant wird. Man sieht auch, dass wir uns auch bei anderen Vergleichen durch das LMM etwas "sicherer" sein können, denn die p-Werte sind niedriger. Hier drückt sich die etwas höhere Power des LMM aus, das alle Trials mit einbezieht (die rmAnova war ja auf der Basis von Mittelwerten berechnet worden). 

### Plots von EMM für die factor_B x factor_A Interaktion

Im Folgenden soll noch einmal die Abhängigkeit von "factor_A"von "factor_B" verdeutlicht werden (das war die "gerade so" signifikante Interaktion, die die rmAnova nicht zeigte).

```{r LMMMarginalMeans2}
emm.lmm.factorAXfactorB <- emmeans(meanRTs.lmm, ~factor_A:factor_B)

ggplot(as.data.frame(emm.lmm.factorAXfactorB), 
       aes(x=factor_B, y=emmean, shape=factor_A)) + 
  geom_point(position = position_dodge(width=0.5), size=3) + 
  geom_errorbar(aes(ymin=asymp.LCL, ymax=asymp.UCL), 
                position = position_dodge(width=0.5), width=0.2) + 
  ylab(label = "reaction time [ms]")

```

Man sieht ein bisschen, dass sich die Stufen von "factor_A" über "factor_B" in leicht unterschiedliche Richtungen bewegen, also eine leichte Interaktion vorhanden ist. 
Auch hier kann  man wieder einen post-hoc-Test rechnen.

### Post-hoc paarweiser Vergleich für die factor_B x factor_A Interaktion
Das Vorgehen ist wiederum analog zu oben. 
```{r LMMPostHoc2}
contrast(emm.lmm.factorAXfactorB, method = "pairwise", adjust = "fdr")
```


## Das generalized linear mixed model (GLMM)
Das GLMM ist in der Anwendung sehr ähnliich wie das LMM, ist aber, wie oben beschrieben, dazu da, um diskrete (z.B. ja/nein)-Antworten auszuwerten.
In der Anwendung ist der wesentliche Unterschied im Code zu dem LMM, dass wir hier einen Parameter setzen: family="binomial". Das sagt dem "mixed"-Befehl, dass es sich um ein GLMM handelt mit einer binären AV. Zusätzlich müssen wir auch eine Methode wählen, um die p-Werte zu schäzten. 
Hier hat man die Wahl zwischen:

- Parametric Bootstrap ("PB"): das ist eine sichere Wahl, die eine gute Schätzung liefert für unterschiedlichste Modelle bzw. Faktorstrukturen. Der Nachteil ist, dass die Schätzung je nach Modell sehr lange dauern kann (tw. mehrere Stunden).
- Likelihood-Ratio-Test ("LRT"): Dieses Verfahren geht deutlich schneller, aber die Schätzungen können ungenau werden, wenn eine einfache random effect struktur gewählt wurde (siehe ?mixed für Informationen) dazu.

Da GLMMs besonder bei "PB" eine deutlich längere Berechnungsdauer als andere  Modelle haben, lohnt es sich mehrere Prozessoren gleichzeitig zu nuten und es macht unter diesen Umständen Sinn, die Ergebnisse abzuspeichern, damit man das Modell nicht erneut berechnen musss, wenn man die R-Session neu startet.

### Der GLMM-Befehl und Output
Wir zeigen hier ein Beispiel, wie das fitten mit mehreren Prozessorkernen und das abspeichern der Daten geht:

```{r, eval=FALSE}
library( parallel ) #für Multicore-Prozesse

nc <- detectCores() - 1 # Anzahl an Prozessoren minus 1

# Um zu sehen, was der Prozess macht, wird Output in eine Textdatei geschrieben.
cl <- makeCluster(rep("localhost", nc), outfile = "cl1.log.txt")

startTime <- Sys.time() #Anfangszeit, um zu sehen, wie lange das Modell läuft
resp.glmm <- mixed( respClean ~ posture * factor_A * factor_B + ( 1 | participant ),
            data = dc, family = "binomial", method = "PB", 
            args_test=list(nsim = 500), check_contrasts = TRUE, cl = cl ) # family = 

endTime <- Sys.time() #Endzeit für den Vergleich mit der Anfangszeit
( endTime- startTime )

#Modellergebnisse abspeichern
save(resp.glmm, file = "resp_glmm.RData")
```


Hier wählen wir noch einen Ansatz mit der "LRT"-Methode:
```{r}
resp.glmm <- mixed( respClean ~ posture * factor_A * factor_B + ( 1 | participant ),
            data = dc, family = "binomial", method = "LRT", 
            check_contrasts = TRUE ) 
```

Als nächstes schauen wir den Output (Anova Table) an:

```{r}
resp.glmm
```

So ähnlich wie bei den RTs sehen wir auch hier einen Haupteffekt von "posture" und "factor_B", sowie eine signifikante Interaktion "posture:factor_B".

### EMM für das GLMM

Auch hier kann man diese Interaktion am besten mit einem Plot und post-hoc Tests nachvollziehen.

Grundsätzlich geht das ähnlich wie bei dem LMM, aber es gibt einen Unterschied: das GLMM hat die Daten auf eine logit-Skala transformiert, die etwas kompliziert ist nachzuvollziehen. Wir können aber in "emmeans" definieren, dass wir die Daten wieder auf die Einheit der Antwort (also zwischen 0 und 1) zurücktransformieren wollen. Als Resultat bekommen wird Werte, die Schätzungen für den Anteil korrekter Antworten (den Wert 1 in unserer Variable) in den Faktorkombinationen sind. Dazu defnieren wir in "emmeans" einfach das Argument: type="response".

```{r}
emm.resp.glmm <- emmeans(resp.glmm, ~posture:factor_B, type = "response")
print(emm.resp.glmm)
```

### Plot der EMM aus dem GLMM
```{r emm-glmm-plot, fig.cap="EMM für die Interaktion im GLMM"}
ggplot(as.data.frame(emm.resp.glmm), 
       aes(x=factor_B, y=prob, color=posture)) + 
  geom_point(position = position_dodge(width=0.5), size=3) + 
  geom_errorbar(aes(ymin=asymp.LCL, ymax=asymp.UCL), 
                position = position_dodge(width=0.5), width=0.2) + 
  ylab(label = "proportion of correct response")
```
### Post-hoc paarweise Vergleeiche für die Interaktion im GLMM
Für die paarweisen Vergleiche kann man identisch vorgehen wie zuvor:

```{r}
contrast(emm.resp.glmm, method = "pairwise", adjust = "fdr")
```

Das einzige, was hier neu ist, ist die Teststatistik "odds.ratio". Eine odds ratio ist das Verhältnis von zwei Wahrscheinlichkeiten. Eine korrekte Antwort ist also bei uncr Level_1 3.3 mal wahrscheinlicher als bei crossed Level_1 und das ist statistisch signifikant. 






<!--chapter:end:08-basic-stats.Rmd-->

# Verknüpfung von Antworten und Reaktionszeiten in ein Maß {#bis-score}

```{r}
load("RTL_beispieldaten.RData")
load("RTL_beispieldaten_clean.RData")

library(tidyverse)
```

## Speed-Accuracy-Tradeoff
Der Zusammenhang von den in der kognitiven Psychologie typischen Verhaltensmaßen Reaktionszeit und (Korrektheit von) Antworten ist ein seit vielen Jahrzehnten viel diskutiertes Thema. Grundsätzlich ist die Idee, dass schwierige Kognitive Prozesse erstens länger dauern und zweitens zu mehr Fehlern führen. Demnach ist zu erwarten, dass eine höhere Anzahl an Fehlern auch mit längeren RTs einhergeht. 
Dieser Gedanke übersieht allerdings, dass sich Reaktionszeit und Fehlerhäufigkeit auch (teilweise mit einer strategischen Komponente) gegeneinander austauschen lassen. Eine Person könnte zum Beispiel sich mehr Zeit lassen beim Antworten mit dem Ziel möglichst wenig Fehler zu machen. (Wenn das aufgeht) hätte sie demnach wenig Fehler, aber lange RT. Eine andere Person könnte riskanter vorgehen und versuchen möglichst schnell zu antworten und dabei in Kauf zu nehmen, dass sie halt mehr Fehler macht. Sie hätte demnach mehr Fehler, aber dafür eine kurze RT. Dieses Phänomen, dass sozusagen Genauigkeit (wenig Fehler machen) gegen Geschwindigkeit "eingekauft" werden kann, ist in der Psychologie als ein *Speed-Accuracy-Tradeoff* bekannt und es ist vielfach belegt, dass Personen das tun und damit sozusagen die Idee, dass Genauigkeit und RT invers zusammenhängen, ein Stück weit untergraben. 

## Der BIS-Score
Es gibt unterschiedliche Ansätze, Reaktionszeiten und Genauigkeiten (Korrektheit der Antworten) miteinander zu verknüpfen und so zu berücksichtigen, dass VPn (in unterschiedlichem Stil) einen Speed-Accuracy-Tradeoff machen. 
Eine neue Methode ist die Berechnung des sogenannten BIS-Scores. 
Wie der BIS-Score berechnet wird und auch, wie er sich von anderen Methoden mit einem ähnlichen Ziel abhebt, ist in dem dazugehörigen Paper nachzulesen:

Liesefeld, H. R., & Janczyk, M. (2019). Combining speed and accuracy to control for speed-accuracy trade-offs(?). Behavior Research Methods, 51(1), 40–60. https://doi.org/10.3758/s13428-018-1076-x

Im Folgenden wird der BIS-Score für die Beispieldaten berechnet. Dazu wird zunächst ein Datensatz gebraucht, der die Mittelwerte der RTs (means) für korrekt und inkorrekt beantwortete Trials enthält. Wir nennen den Datensatz mal "dc.m".

```{r BIS-data}
dc.m <- dc %>% group_by( participant, posture, factor_A, factor_B ) %>%
  summarise(
    sdRtCorr = sd( rtClean[ respClean == 1 ] ),
    rtCorr = mean( rtClean[ respClean == 1 ] ),
    rtAll = mean( rtClean ),
    corrCount = sum( respClean == 1 ),
    totalCount = n(),
    pc = corrCount / totalCount *100
  )

```
Jetzt können wir den BIS-Score berechnen. 

```{r}
# BIS
dc.m <- dc.m %>% group_by( participant ) %>% mutate(
  zRt = ( rtCorr - mean( rtCorr ) ) / sqrt( sum ( ( rtCorr - mean( rtCorr ) ) ^ 2 ) / n() ),
  zPc = ( pc - mean( pc ) ) / sqrt( sum ( ( pc - mean( pc ) ) ^ 2 ) / n() ),
  bis = zPc - zRt
)

```

Dabei sollte beachtet werden, dass der BIS-Score innerhalb der Versuchsperson berechnet ist und damit normalisiert wird. Das heißt, der BIS-Score drückt die Unterschiede zwischen Bedingungen aus und nicht die UNtersdchiede zwischen Personen generell. ODer in anderen Worten: die Summe der BIS-Scores innerhalb jeder Person über die Bedingungen ist (bis auf Rundungsfehler) immer gleich und hat den Wert 0.

Das kann man einfach überprüfen:
```{r}
dc.m %>% group_by( participant ) %>% 
  summarise(meanBIS=round(mean(bis)))

```

## Analyse des BIS-Scores
Der BIS-Score ist metrisch und normalerweise parametrisch. Es liegt pro Person und Zelle genau ein Wert vor. 
Eine gute Wahl für die statistische Analyse ist die rm-Anova, die wir in Teil \@ref(basic-stats) schon vorgestellt haben. 


```{r}
BIS.rmanova <- aov_ez(id = "participant", dv = "bis", data = dc.m, within = c("factor_A", "factor_B", "posture"))

BIS.rmanova
```

So ähnlich wie bei der rm-Anova für die RTs sehen wir auch hier Haupteffekte für factor_B, für posture und für die factor_B:posture Interaktion.

## Visualisierung der Ergbenisse des Modells

Wieder können wir uns die EMM graphisch anschauen. 
```{r rmAnova_bis}
emm.bis.factorBXposture <- emmeans(BIS.rmanova, ~factor_B:posture)
```


```{r EMM-bis, fig.cap="EMM zur für factor_B und posture"}
ggplot(as.data.frame(emm.bis.factorBXposture), 
       aes(x=factor_B, y=emmean, color=posture)) + 
  geom_point(position = position_dodge(width=0.5), size=3) + 
  geom_errorbar(aes(ymin=lower.CL, ymax=upper.CL), position = position_dodge(width=0.5), width=0.2) + 
  ylab(label = "BIS score")
```

## Post-hoc Vergleich zum BIS-Score
Der paarweise Vergleich sieht wie folgt aus:
```{r}
contrast(emm.bis.factorBXposture, method = "pairwise", adjust = "fdr")
```

Wir hoffe, Sie haben gesehen, dass selbst diese etwas komplexeren Auswertemethoden in ihrer Implementierung in R und der Interpretation nicht all zu schwer sind. 

Im nächsten Teil wollen wir noch einige Möglichkeiten zeigen, wie man "schöne" Abbildungen und Tabellen machen kann.



<!--chapter:end:09-bis-score.Rmd-->

`r if (knitr::is_html_output()) '
# References {-}
'`

<!--chapter:end:10-references.Rmd-->

